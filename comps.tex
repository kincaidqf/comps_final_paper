\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% need to explicitly declare biblatex package to avoid conflict with bibtex
\usepackage[style=numeric,sorting=nyt]{biblatex}
\addbibresource{references.bib}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series)
    /Author (Kincaid Fries)
}

% set the title and author information
\title{TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series}
\author{Kincaid Fries}
\affiliation{Occidental College}
\email{kfries@oxy.edu}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Background}
Machine learning (ML) has been widely adopted in stock market prediction due to the ability to model complex, multivariable relationships from 1000s of daily stock price datapoints. In contrast, \textcite{baltazar2020sustainableeconomies} note that macroeconomic ML applications are much less reliable due to complicated surveying that takes months of research to produce even a single viable measurement. Additional challenges like varied calculation methods, unavailability of data for certain time periods, and changes in indicator definitions across countries make it hard to train macroeconomic models without overfitting to small datasets. I want to investigate synthetic data as an alternative avenue for allowing researchers to create realistic simulations of economic behavior without relying solely on historical data.

\subsection{Goal}
The goal of my senior comprehensive project is to evaluate whether a machine learning model trained on small macroeconomic datasets can be used to generate realistic synthetic time series. I hope to address the issue of relatively limited data availability per country by taking training data from multiple countries. My project will explore whether synthetic time-series could potentially address the problem of lack of data availability, and serve as a valid counterpart to real economic data in forecasting applications.

Data availability is an increasingly prevalent issue as the world embraces machine learning. The demand for training data is already outpacing data availability, with major data companies like Meta and OpenAI turning to piracy as a solution. If we had the power to take small datasets and extend them, or create realistic alternative data using synthetic generation, then these problems could be solved. Security, privacy, and copyright could be better preserved while still having access to the quantity of data needed to train machine learning models. 

\subsection{Approach}
I will implement a TimeGAN model, which combines a supervised autoencoder to learn temporal dynamics with a generative adversarial network (GAN) to generate realistic synthetic data \textcite{yoon2019timeseriesgenerative}. The model will be trained using macroeconomic data taken from developed, free-market economies, to ensure there is consistency in the underlying macroeconomic trends it identifies. I will compare the properties of the synthetic data with real data using a number of statistical tests on distributional similarity, temporal structure, and novelty. I will examine the performance of a forecasting model using the train-synthetic test-real (TSTR) method to assess the potential viability of synthetic data in downstream ML training. Through consideration of both statistical factors and forecasting performance, I will develop a comprehensive framework for assessing the viability of the synthetic data. I will then refine the model, and explore optimization over small limited training data.

\section{Technical Background}
\subsection{Macroeconomic Indicators}
Macroeconomic indicators are data points which quantify the health of an entire economic market at a country-wide level. They are impactful across all levels of society and business, and are even politically salient \cite{ravallion2021macroeconomicmisery}. The macroeconomic indicators that I will focus on will be gross domestic product growth (GDP), inflation rate (I), unemployment rate (R), and population growth (P), which were selected with insight from the research of  \textcite{ravallion2021macroeconomicmisery} and \textcite{baltazar2020sustainableeconomies}. 

\subsubsection{Selected Indicators}
\begin{description}
    \item[\textbf{GDP:}] Total monetary value of all final goods and services in an economy. I will be using the rate of GDP growth expressed as a percentage change in GDP for this project.
    \item[\textbf{Inflation Rate:}] Percent change in price of a pre-defined ‘basket’ of common household goods. Represents the change in currency value over time.
    \item[\textbf{Unemployment Rate:}] Percent of the labor force who is unemployed and actively seeking work. The efficient unemployment rate is typically around 5\% \cite{voxeuUnemployment}.
    \item[\textbf{Population Growth:}] Percent change in the population. Though not typically considered a macroeconomic indicator, it helps control for other indicators. If GDP and unemployment rates both go up, a possible explanation would be an increase in population.
\end{description}

\subsubsection{Relevance of Macro-Indicators}
Macroeconomic indicators must be viewed in conjunction with each other, as no individual measure can capture the state of the economy \cite{ravallion2021macroeconomicmisery}. Whenever one of the indicators changes, it will surely have an impact on the other indicators and the overall state of the economy. This project will look at each of the indicators outlined above in order to capture a holistic picture of the economy. Macroeconomic forecasting is primarily used by banks and the federal government. For example, \textcite{baltazar2020sustainableeconomies} detail how macroeconomic models can be used by banks to find their best response to economic shocks, potentially helping to handle situations like the 2008 financial crisis.

\subsection{Relevent Machine Learning Terminology}
\begin{description}
    \item[\textbf{Supervised learning}] uses labelled data to guide the models learning but is not well suited for time series \cite{yoon2019timeseriesgenerative}.
    \item[\textbf{Unsupervised learning}] identifies patterns without the guidance of labels.
    \item[\textbf{Recurrant Neural Networks}] are artificial neural networks that pass data between neurons with a parameter allowing nodes to observe both the current and previous data state.
    \item[\textbf{Generative Adversarial Networks (GANs)}] consist of a generator neural network competing with a discriminator \cite{awsGAN}. The generator wants to beat the discriminator by producing data it can’t tell is fake    \item[\textbf{Latent Space}] refers to the compression of input data into a lower dimensional space that preserves essential features of the data’s structure \cite{ibmLatentSpace}.
    \item[\textbf{Autoencoders}] learn to encode data to a latent space and then reconstruct it, allowing them to learn the structure of the data in the process \cite{ibmLatentSpace}.
\end{description}

\subsection{Choosing TimeGAN}
\label{sec:choosing-timegan}
TimeGAN stands for Time-Series Generative Adversarial Network, a proprietary machine learning model created by \textcite{yoon2019timeseriesgenerative} combining supervised, unsupervised, and adversarial learning to encourage the model to follow the temporal dynamics of training data. The goal of their research stems from the inadequacy of other machine learning models to generate synthetic data that accurately takes temporal correlations into account \cite{yoon2019timeseriesgenerative}. 
TimeGAN is trained on a 3D dataset of multiple independent time-series, shaped as $[N, L, D]$, where $N$ is the number of sequences (e.g. countries), $L$ is the number of time steps, and $D$ is the number of dimensions (e.g., GDP, CPI). This makes TimeGAN perfect for my goal of training a model on multiple countries data.

\subsection{TimeGAN Architecture}
The main components of the TimeGAN architecture are pictured in Figure \ref{fig:timegan-architecture}. Notably the supervisor is not pictured, but operates in the latent space between the embedder and recovery networks. $\delta\mathcal{L}$ represent the loss values, training parameters represented with $\theta$.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{TimeGAN training.png}
    \caption{Components of TimeGAN Training Architecture. Adapted from \textcite{yoon2019timeseriesgenerative}.}
    \label{fig:timegan-architecture}
\end{figure}

\begin{description}
    \item[\textbf{Embedding Network}] (\textit{e}, \ref{fig:timegan-architecture}) encodes training data into a lower-dimensional latent space. 
    \item[\textbf{Recovery Network}] (\textit{r}, \ref{fig:timegan-architecture}) learns data relationships by decoding the data embedded in the latent space back into the original data-space. The combined embedding and recovery networks form an autoencoder in TimeGAN.
    \item[\textbf{Supervisor Network}] (not pictured in diagram) works in the latent space learning the pattern between the next step hidden state by observing the previous. The supervisor provides synthetic latent transitions to the Generator network, giving the Generator per-timestep guidance on how to generate data with accurate temporal transitions.
    \item[\textbf{Generator Network}] (\textit{g}, \ref{fig:timegan-architecture}) generates sequences (time-series) from a random distribution of Gaussian noise vectors. Through competition with the discriminator and guidance from the supervisor, the generator learns how to convert random input into a realistic distribution.
    \item[\textbf{Discriminator Network}] The discriminator network (\textit{d}, \ref{fig:timegan-architecture}) is the adversarial part of the model which attempts to distinguish between real and synthetic generations.
\end{description}

What makes this model unique is that the autoencoder (\textit{e} and \textit{r} in Figure \ref{fig:timegan-architecture})  learns latent representations, while the supervisor learns time-step shifted latent transformations (the transition from latent representation at time \textit{t} to time \textit{t+1}). The latent patterns help the generator to create realistic data for the next time step based on the current time step. The adversarial component of the discriminator provides further reinforcement on generating quality synthetic sequences. The TimeGAN iteratively improves how it encodes features and generates representations during training \cite{yoon2019timeseriesgenerative}. The combination of GAN and autoencoder architecture make a TimeGAN the perfect model for my project: designed to generate realistic time series data.

\subsection{Loss Functions}
\subsubsection{Autoencoder (reconstruction) Loss}
The autoencoder uses mean squared error; a standard loss function in machine learning. The result is a quantification of the average difference between the input into the embedder and output from the recovery network, allowing the autoencoder portion of the model to learn better how to encode and recover data from the latent space.

\begin{equation}
    \mathcal{L}_{\text{AE}} = \frac{1}{L-1}\sum_{t=0}^{L-1}\mathbb{E}\left[\left\|X_t - \hat{X}_t\right\|^2\right]
\end{equation}

\(X_t\) is the real data fed into the embedder at time step \(t\), and \(\hat{X}_t\) is the data recovered from the latent embedding by the recovery network. The sum of squared error terms is averaged over the number of time steps $L$ (the length of the series).

\subsubsection{Supervised Loss}
The supervised loss is also a mean squared error computed over latent representations shifted by one time step. Here, $\hat{H}_{t+1}$ denotes the supervisor network's prediction of the latent state at time $t+1$, and $H_{t+1}$ denotes the true latent state at time $t+1$. The supervised loss encourages the model to learn realistic temporal transitions in the latent space.

\begin{equation}
    \mathcal{L}_{\text{Sup}} = \frac{1}{L-1}\sum_{t=0}^{L-2}\mathbb{E}\left[\left\|H_{t+1} - \hat{H}_{t+1}\right\|^2\right]
\end{equation}

\subsubsection{Generator Loss}
The generator loss has two components. First there is the adversarial loss, which is the component that comes from the discriminators ability to correctly identify the real data from the synthetic. Second is the supervised loss, which is weighted by a gamma coefficient that simply represents the amount of emphasis placed on supervised versus adversarial loss. 

\begin{equation}
        \mathcal{L}_{\text{G}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right] + \gamma \mathcal{L}_{\text{Sup}}
\end{equation}

The adversarial loss uses logits, which are raw unbounded output of the discriminator on the synthetic latent sequence $\hat{H}$. Logits are passed through the sigmoid function which compresses it into a probability from $[0,1]$. $\sigma\!\big(D_{\text{logit}}(\hat{H})\big)$ is the probability of seeing a logit value of $D_{\text{logit}}(\hat{H})$. The logit-sigmoid process internally makes the derivatives of the loss function easier to derive. The generator is trying to minimize the probability that its synthetic data is labelled synthetic by the generator, so the adversarial element of this loss function is negative.

\subsubsection{Discriminator Loss}
The discriminator loss function uses a standard function for yes no classification called binary cross-entropy. Binary cross-entropy is made up of two components, the probability score of assigning fake data the label of fake, and the probability score of assigning real data the label of fake. 

\begin{equation}
    \mathcal{L}_{\text{D}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(H\right)\right)\right)\right] - \mathbb{E}\left[\log\left(1 - \sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right]
\end{equation}

Both the supervisor and discriminator use expectation notation $\mathbb{E}[\cdot]$, which denotes the integral over the data probability distribution, e.g.\ $\mathbb{E}_{x\sim p(x)}[f(x)]=\int f(x)\,p(x)\,dx$, and in practice is approximated by a finite-sample average over a mini-batch: $\mathbb{E}_{x\sim p(x)}[f(x)]\approx\frac{1}{m}\sum_{i=1}^m f(x_i)$, where $m$ is the mini-batch size and $x_i$ are the samples.

\section{Prior Work}
The original TimeGAN implementation arose due to the fact that GANs “do not adequately attend to temporal correlations”, while supervised models are “inherently deterministic” (\cite{yoon2019timeseriesgenerative}, 1) . \textcite{yoon2019timeseriesgenerative} proposal for addressing this disparity was to create an interwoven framework that provides both the benefits of supervised models and the adversarial component of GANs. My project is based on the TimeGAN model framework, which stood out to me because it is trained on batches of data that make it ideal for learning macroeconomic conditions across different countries. Additionally the combination of adversarial and supervised objectives offered the possibility to support learning on small datasets without overfitting. 

My project draws primarily on four studies that reflect the technical, theoretical, and evaluative aspects of synthetic data generation and machine learning applications in economics. The first applies GANs to generating synthetic financial scenarios (\textcite{rizzato2023generativeadversarial}). The next applies a macroeconomic model to loan default rate predictions (\textcite{baltazar2020sustainableeconomies}). Finally, \textcite{yuan2024multifacetedevaluation} and \textcite{livieris2024anevaluationframework} provide a reference for how to go about my evaluation metrics and results discussion.

\subsection{Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation}
\textcite{rizzato2023generativeadversarial} propose a GAN approach to synthetic financial scenarios called Jinkou. Jinkou was designed to generate synthetic time series datasets on the movement of equities under different macroeconomic conditions \cite{rizzato2023generativeadversarial}. Unlike the original TimeGAN paper, they introduce a macroeconomic element which offers a similar conceptual approach and methodology to my own goals for this project. 

\subsubsection{Technical Approach}
They took data on equities movements and augmented it with global state variables describing macroeconomic conditions \cite{rizzato2023generativeadversarial}. Their proprietary Jinkou method uses a combination of a bidirectional GAN (BiGAN) and conditional GAN (cGAN) to improve the probability distribution of the synthetic data \cite{rizzato2023generativeadversarial}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{bigan_vs_cgan.png}
    \caption{Visual comparison of BiGAN and Conditional GAN training architectures. Adapted from \textcite{rizzato2023generativeadversarial}.}
    \label{fig:bigan-vs-cgan}
\end{figure}

The BiGAN maps latent space to the data space with an autoencoder, similar to the embedder and recovery networks of the TimeGAN, and produces a synthetic dataset. The cGAN refines the results of the BiGAN by mapping each datapoint \textit{y} to a probability score \textit{p} drawn from the dataset distribution of \textit{P} \cite{rizzato2023generativeadversarial}. They experiment with four different market conditions (bull market, bear market, volatile market, and debt crisis) and evaluate the change in stock price from start to end of the time series for real vs synthetic data \cite{rizzato2023generativeadversarial}. They found similar values for synthetic vs real data, typically with a difference between [-1,2]\%, indicating that their synthetic data was accurate to real data predictions.

\subsubsection{Key Takeaway}
Specific to my project, \textcite{rizzato2023generativeadversarial} found the use of GAN objectives to be effective for generating synthetic time-series, and were able to compare that synthetic data to real data with a satisfactory degree of accuracy. Their results validate the importance of macroeconomic indicators in financial modelling.

\subsection{Sustainable Economies: Using a Macroeconomic Model to Predict How the Default Rate is Affected Under Economic Stress Scenarios}
\textcite{baltazar2020sustainableeconomies} build a macroeconomic stress-testing framework to predict loan default rates under hypothetical, simulated macroeconomic conditions. A framework they argue would allow banks and policy makers to improve their decision making in economic downturns \textcite{baltazar2020sustainableeconomies}. They use a predefined macroeconomic model rather than machine learning, having that model generate random Monte Carlo simulations \textcite{baltazar2020sustainableeconomies}.\\

\textcite{baltazar2020sustainableeconomies} acknowledge the problem of small datasets for macroeconomic forecasting . Their study validates the use of GDP and inflation rate for understanding economic conditions, the downstream effects of macroeconomic conditions on finance and the economy, and the necessity and potential applications of synthetic macroeconomic data in augmenting existing datasets \cite{baltazar2020sustainableeconomies}.

\subsection{A Multi-Faceted Evaluation Framework for Assessing Synthetic Data}
This study, by \textcite{yuan2024multifacetedevaluation}, proposes a novel method for the evaluation of synthetic datasets, which they call SynEval. Their evaluation method has three dimensions–fidelity, utility and privacy–of which I am interested in ‘utility’, their measure of the applicability of the synthetic data \cite{yuan2024multifacetedevaluation}.

\subsubsection{Technical Approach}
\indent{}\textcite{yuan2024multifacetedevaluation} refer to the ‘utility’ of a synthetic dataset as its ability to be used downstream for machine learning. Downstream machine learning refers to the application of the generated data to training/testing machine learning models. The utility evaluation they use is Train-Synthetic-Test-Real (TSTR) \cite{yuan2024multifacetedevaluation}. They perform a study on synthetic data generated by popular LLMs (ChatGPT 3.5, Claude 3 Opus, and Llama 2 13B) \cite{yuan2024multifacetedevaluation}. They then trained four sentiment classification models; one on a real dataset, and the other three on the synthetic datasets produced by the LLMs \cite{yuan2024multifacetedevaluation}. The sentiment classification models were evaluated on unseen test data, with Mean Absolute Error (MAE) and percentage accuracy of correct classifications used to evaluate the performance of each model. 

\subsubsection{Key Takeaway}
In evaluating the synthetic datasets on downstream machine learning performance, they found similar results between models trained on synthetic and real data, which demonstrates that synthetic data can be effectively used on downstream machine learning tasks \cite{yuan2024multifacetedevaluation}. Using downstream machine learning as an evaluation metric for the quality of synthetic data validates my plan of using a forecasting model trained on synthetic data as an evaluation metric. 

\subsection{An Evaluation Framework for Synthetic Data Generation Models}
\textcite{livieris2024anevaluationframework} discuss a number of statistical methods specifically for evaluating synthetic data. They compare data generated by five synthetic data generation models using standard tests. The two tests they used which I find most relevant to my project were Wasserstein-Cramer's V-test and novelty test \cite{livieris2024anevaluationframework}. 

\begin{itemize}
    \item \textit{Wasserstein-Cramer's V Test} combines the Wasserstein distance (WD) with Cramer’s V to account for the distributions across different variables \cite{livieris2024anevaluationframework}. The test evaluates if synthetic data matches the distribution of real data accounting for shape and variance, with low scores being better. Since Wasserstein-Cramer's V Test scores are not bounded to a range, it is somewhat arbitrary determining what is a good score or not.
    \item \textit{Novelty Test}: measures the ratio of unmatched instances between the synthetic and real dataset, where an instance is considered match if $\left| s_i - r_i \right| < \alpha$, with $s_i$ representing a synthetic datapoint, $r_i$ representing a real point, and $\alpha$ being some predefined threshold \cite{livieris2024anevaluationframework}. 
\end{itemize}
\subsubsection{Key Takeaway}
I will use Wasserstein Distance as part of my distributional evaluation. Implementing the combination with Cramer’s V-test is not a commonly accepted practice and makes the results a bit less interpretable, so I will focus just on Wasserstein Distance rather than the combined test. Additionally, I will implement the novelty test through a simple nearest neighbors Euclidian distance calculation.

\section{Methods}
\subsection{Sourcing Macroeconomic Data}
For macroeconomic training data I used the World Bank’s World Development Indicators (WDI) online database \cite{worldbank_wdi_2024}. I gathered a training dataset of the USA, Japan, Australia, UK, Canada, South Korea, Denmark, Germany and France–countries which I chose for having developed economies with reliable data going back 50+ years.

\subsection{Data Pre-Processing}
My data points of GDP growth, inflation rate, unemployment rate, and population growth were chosen deliberately to represent a snapshot of countries macroeconomic conditions without having collinearity between any of the features. GDP growth gives an idea of how the dollar quantification of the economy changes annually, while inflation rate helps control for the actual value of those dollars change adjusted for inflation. Unemployment rate along with inflation rate are considered two of the most revealing macroeconomic indicators, while population growth helps to control for some of the variation in unemployment and GDP growth.

Everything was processed using a standard min-max scalar compression to [-1,1], with the range of the data being saved in order to decompress the synthetic series at the end for comparison.

The most significant pre-processing decision I made was to divide the datasets into batches of sliding windows. With only 9 training countries I would have had a [9, 56, 4] training dataset (see \ref{sec:choosing-timegan} for training shape). This was simply too small of a dataset to do any real training on. As a way of combatting this, I broke each time series into a set of sliding windows (see Figure \ref{fig:sliding-window}). A window represents a sequence of a chosen length; in this case a 24 year period that is long enough to observe macroeconomic changes while allowing for the data to be divided up into many sequences. My stride length for the sliding windows was 1, so the start time of each window was incremented by 1. After dropping null values I was left with 155 windows available for training.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{sliding_windows.png}
    \caption{Visualization of sliding window approach to data batching. Colored boxes represent windows of length 4, with stride length 1. Adapted from \textcite{zhan2024versatile}.}
    \label{fig:sliding-window}
\end{figure}

Though windows will overlap, this method allows the network to learn the dynamics of short sequences through exposure to a much higher volume of sequences. With a [9, 56, 4] training dataset the network is exposed to 9x56=504 training steps. On a [64, 24, 4] training batch size as I used for many of my tests, the model is exposed to 64x24=1536 training steps. Even though there will be overlap, using 64/155 randomly sampled batches at a time minimizes overlap, and the increased training steps is worth the slight increase in risk of overfitting.

\subsection{TimeGAN Implementation}
For the actual TimeGAN model implementation itself I followed the original repository with some adaptation to my small dataset use case and different versioning requirements. The model is built on the TensorFlow library, with the data represented as tensors and the neural networks being built into as a tensor graph. Due to updates since the original TimeGAN paper, some structural changes were needed to switch to TensorFlow Keras versioning from the original. These changes affect the way some calls are made but do not affect the performance/implementation of the model.

For clarity's sake, I decided to move the actual neural cell instantiation into a helper function rather than directly call inside the neural networks as done in the original repository. This allowed me to treat RNN cell type as an adaptable parameter rather than a constant. 

I used a Xaviar Initialization for the weight matrix of the neural connections. A Xaviar Init takes an input shape and outputs a random weight matrix with slight noise added for connecting the nodes within that shape. This introduces some slight variation and randomness into the weight matrix for each training iteration of the neural networks. With a small dataset the goal was to maximize justified, structured randomness to reduce overfitting. 

I use an Adam Optimizer on the autoencoder, generator, and discriminator. The Adam Optimizer is an adaptive learning rate optimization algorithm (standard TensorFlow function) that adjusts the learning rate for features during training if it detects that certain features are being over or under emphasized. 

\subsection{Study Method}
I will conduct evaluation on the synthetic generated data using the evaluation metrics \ref{sec:evaluation-metrics}, and iteratively adjust the model based on performance. My goal is to have a clear improvement between the performance of the stock model versus a final, tuned version, and be able to explain what changes I made and why they specifically are impactful to my small training set. Positive change could apply to improvement in the evaluation metrics or to an improvement in model health as indicated by loss values. Any parameter adjustments which have a notably positive impact I will treat as part of the new baseline, and will carry forward into the next trial.

After implementing the model and performing my iterative experiments over parameter adjustments, I hope to have a set of justified, meaningful changes that correlate to improved synthetic data quality. Through this approach I hope to justify that it is possible to optimize TimeGAN over small datasets to effectively generate synthetic data, as well as identify areas for future improvements in TimeGAN performance.

\section{Evaluation Metrics}
\label{sec:evaluation-metrics}

My goal is to statistically quantify the ability of the synthetic data to maintain distributional and temporal features while not overfitting training data. To achieve this I selected a number of evaluation metrics, as well as implemented a train-synthetic test-real approach as guided by prior work.

\subsection{Feature Distribution: Kolmogorov-Smirnov}
In order to evaluate the per-feature distribution of my synthetic data I will use the Kolmogorov-Smirnov (KS) test. The KS test is a cumulative frequency distribution (CFD) comparison test, used by \textcite{bourou2021reviewtabulardata} to evaluate the performance of GANs in generating synthetic data. The KS test finds the maximum vertical distance between two CFDs – in this case a real CFD and synthetic CFD (see Figure \ref{fig:ks-test}). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{KS2_Example.png}
    \caption{Visualization of Kolmogorov-Smirnov test between cumulative frequency distributions. From \textcite{wikipedia_kstest}.}
    \label{fig:ks-test}
\end{figure}

KS uses the maximum distance calculation to perform a hypothesis test that the two distributions are not significantly different. If the p-value calculated by the test is less than your desired significance level $\alpha$, then you can conclude that the sample distribution does not come from the original distribution. The KS test was run on each feature in the dataset, to give a sense of distributional similarity on a per-feature level.

\subsection{Feature Distribution: Wasserstein Distance}
The distribution test I will use is Wasserstein Distance, inspired by \textcite{livieris2024anevaluationframework} approach to synthetic data evaluation. WD measures the absolute difference (or cost) to transcribe one distribution onto another. Unlike the KS test which only looks at the individual point of highest difference, the WD looks at the total difference between CFDs. Like the KS test, I use a WD test on a per-feature basis to help evaluate whether one distribution came from another, in my case whether the synthetic distribution was derived from the real distribution. 

Using WD and KS tests I will measure both the maximum and total cumulative difference between real-synth distributions. Since I know that the synthetic distributions came from the real, I will not be using these measurements for hypothesis testing reasons, but simply as a metric about the difference between distributions.

\subsection{Cross Feature Correlation}
I will use the Frobenius norm of correlation matrices to calculate cross feature correlation, inspired by \textcite{marti2020corrgan}, who applies adversarial techniques to sampling on financial correlation matrices. A correlation matrix is a DxD matrix of features with each feature’s correlation coefficient being stored in the corresponding box. In my case the correlation matrices would look like the following:

\begin{table}[ht]
    \centering
    \caption{Correlation Matrix of Macroeconomic Indicators}
    \label{tab:correlation-matrix}
    {\small%
    \setlength{\tabcolsep}{4pt}%
    \renewcommand{\arraystretch}{0.95}%
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c c}
        \hline
         & \textbf{GDP Growth} & \textbf{Inflation (I)} & \textbf{...} \\
        \hline
        \textbf{GDP Growth}     & 1                          & $\mathrm{corr(GDP,I)}$   & ... \\
        \textbf{Inflation (I)} & $\mathrm{corr(I,GDP)}$ & 1                          & ... \\
        \textbf{...}            & ...                        & ...                        & ... \\
        \hline
    \end{tabular*}%
    }%
\end{table}


A correlation matrix for both real vs real and for synthetic vs synthetic will be compared using the Frobenius norm, which calculates the total difference between matrices in the same conceptual manner as calculating Euclidean distance between vectors. Though there is not a standard to aim for in terms of Frobenius score, lower is better as it implies that the relationships between features are more similar. A lower Frob norm implies that the datasets have more realistic inter-feature relationships.

\subsection{Autocorrelation}
In order to more specifically consider temporal features, I included an autocorrelation function comparison (ACF). The ACF measures the correlation between a function and a time delayed version of itself. I compute the ACF score per feature for both the synthetic and real data. I then find the root mean squared error (RMSE) between both the average ACF score across features, and the per feature ACF score of synthetic and real datasets. Using RMSE ensures that larger differences are more heavily penalized. Since I am evaluating multiple features, even if most features have a good autocorrelation score it is alarming if one of them has a particularly bad score, which will be heavily weighted by the RMSE comparison. A lower average ACF RMSE indicates that the temporal features are more similar between the synthetic and real datasets.

\subsection{Predictive (TSTR) Performance}
As a test of downstream machine learning applicability, I used a train-synthetic test-real method of evaluation \cite{yuan2024multifacetedevaluation}. The premise is to evaluate whether synthetic data could be used as a viable proxy for real data by training a simple model purely on synthetic data, then evaluating its performance on real data. For my application, I used a stock linear regression model from TensorFlow trained on synthetic data to predict the next next timestep. The model’s performance was then evaluated using the real data as a test batch, with the difference between actual and predicted quantified using mean squared error. The idea is that using real or synthetic data for training should be irrelevant as long as the synthetic data is realistic. 

\subsection{Novelty}
The final essential dimension of synthetic data evaluation is novelty \cite{livieris2024anevaluationframework}. I use a K-Nearest Neighbors (KNN) test to find the nearest matched points between the synthetic and real data. First I normalize the distributions to 0 to ensure that different order-of-magnitude features do not skew the results. For every point in each distribution I find its nearest neighbor in terms of absolute Euclidean distance. I run this comparison from both synthetic to real and real to synthetic, which allows me to obtain a KNN asymmetry score (difference between synth-to-real and real-to-synth KNN mean) which tells me the direction the distribution is skewed toward. Ideally KNN scores would be high (indicating plenty of novelty) but KNN asymmetry would be low, meaning the new points are equally distributed. A high KNN score represents that the synthetic data is introducing novelty into the synthetic data it is generating, which if possible while still maintaining distributional/temporal dynamics is the exact goal of this model.

\section{Results and Discussion}
\subsection{Baseline Model Evaluation}
My first test of the complete model used only the set of default parameters listed in Table \ref{table:default-params} (Appendix \ref{appendix:architecture-overview}). As a baseline I ran the evaluation metrics over comparisons between real and synthetic data, and between real training and withheld data. This established a benchmark for the evaluation methods.

Throughout testing the only evaluation metric which wasn’t informative was the Kolmogorov--Smirnov (KS) test. Even the real--real comparison in Table \ref{table:baseline-v0-results} reports a KS Max of 0.875, which corresponds to an effectively zero p-value. I attribute this to differing starting conditions across countries and periods. Windows that include economic shocks for example would have significantly different CDFs, causing high KS distances even between real samples. Since two distributions can be real and have significantly different underlying macroeconomic scenarios, KS is of limited use here.

\begin{table}[t]
    \centering
    \caption{Comparison of evaluation metrics between the real--real baseline and the initial synthetic baseline (v0).}
    \label{table:baseline-v0-results}
    {\small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{0.95}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c}
        \toprule
        \textbf{Metric} & \textbf{Real Baseline} & \textbf{v0 Synthetic} \\
        \midrule
        KS Max              & 0.875  & 0.975  \\
        WD Mean             & 0.174  & 0.206  \\
        WD Max              & 0.341  & 0.384  \\
        Frob Diff           & 1.438  & 3.432  \\
        ACF RMSE            & 0.174  & 0.155  \\
        TSTR MSE Mean       & 0.037  & 1.096  \\
        KNN Mean S--R       & 14.361 & 12.728 \\
        KNN Mean R--S       & 13.725 & 15.755 \\
        KNN Asymmetry       & 0.318  & -3.144 \\
        \bottomrule
    \end{tabular*}
    }
\end{table}

Overall my synthetic baseline (v0) test performs similarly to the real-real comparison except for on realism and predictive scores. Both WD mean and max are higher in v0, but are relatively similar (Table \ref{table:baseline-v0-results}). This indicates that the model is getting rough distributional shapes, but is performing poorly at more extreme values (higher WD Max). Thus marginal distributions are not well matched, but distributional closeness is not drastically different from the real baseline.

There was a large disparity in performance in the Frobenius difference in correlation matrices. The real baseline had a Frob score of 1.438, while v0 had a significantly higher score of 3.432 (Table \ref{table:baseline-v0-results}). This indicates that the v0 synthetic data does not at all mimic the inter-feature relationships of the real data well. Surprisingly v0 has slightly better ACF RMSE than the real-real baseline. Most likely this is due to the fact that early GAN outputs could be oversmoothed, generating series without much temporal variation that creates artificially low autocorrelation, which would explain some of the poor performance in Frobenius difference. 

The most important difference between the two tests is the TSTR performance. The TSTR MSE of the v0 test was 1.096, significantly worse than the real baseline of 0.037 (Table \ref{table:baseline-v0-results}). The poor predictive score indicates that the synthetic data cannot reasonably be used to predict real data relationships. The synthetic data is not realistic enough for any sort of downstream application, which is how \textcite{yuan2024multifacetedevaluation} defines the real-world utility of synthetic data.

In terms of novelty it seems however that a score of about 14 is realistic given the results of the real-real comparison. What stands out is that for the v0 test, there was a much higher degree of asymmetry (-3.144 vs 0.318) between the real-fake and fake-real KNN means (Table \ref{table:baseline-v0-results}). This implies that the synthetic data is asymmetric, meaning that v0 synthetic samples cluster in certain ranges (lower synthetic to real score) but fail to cover the full range of real data (higher real to synthetic score).

\subsection{Final Results: Tuned Model}

From my initial baseline to my final trial (v14) there was substantial improvement across distributional, temporal, discriminative, and most importantly downstream application performance. See Appendix \ref{appendix:further-results} for further discussion of intermediate results and trials. 

\begin{table}[t]
    \centering
    \caption{Performance improvements from the initial model (v0) to the tuned model (v14).}
    \label{table:v0-v14-improvement}
    {\small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{0.95}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c c}
        \toprule
        \textbf{Metric} & \textbf{v0} & \textbf{v14} & \textbf{Improvement} \\
        \midrule
        KS Max            & 0.975  & 0.975  & 0\%      \\
        WD Mean           & 0.206  & 0.183  & 12.56\%  \\
        WD Max            & 0.384  & 0.362  & 6.07\%   \\
        Frob Diff         & 3.432  & 2.621  & 30.94\%  \\
        ACF RMSE          & 0.155  & 0.287  & --45.99\% \\
        TSTR Mean         & 1.096  & 0.588  & 86.39\%  \\
        KNN Mean S--R     & 12.728 & 13.202 & --       \\
        KNN Mean R--S     & 15.755 & 15.611 & --       \\
        KNN Asymmetry     & -3.144 & -2.409 & 30.51\% \\
        \bottomrule
    \end{tabular*}
    }
\end{table}

The final incremental change from v10 to v14 was increasing hidden dimensions from 24-32.  The hidden dimensions directly control the capacity of the embedder, supervisor, generator, and recovery networks exposure to previous hidden states. The increased hidden dimensions better allow the supervisor to model lagged relationships, which helps inform how the generator generates realistic synthetic time steps. 

Compared to v0, WD mean and max showed a moderate improvement, of 12.56\% and 6.07\% respectively–indicating that the model matches the shape of the real distributions better, and that distributions are not as narrowly clustered as in v0 (Table \ref{table:v0-v14-improvement}). Major improvement was seen in Frobenius difference, a 30.94% improvement over v0, indicating that feature to feature relationships were far more accurate than initially (\ref{table:v0-v14-improvement}). This implies that the synthetic data has much more realistic features to feature relationships than the baseline. 

The only metric where v14 actually performed worse than v0 was the autocorrelation structure (ACF RMSE). The change from an ACF RMSE of 0.155 in v0 to 0.287 in v14 indicates that there was more temporal volatility in v14 (Table \ref{table:v0-v14-improvement}). While this is a significantly worse score, in terms of my goals it is not necessarily concerning. The initial ACF RMSE score for v0 was lower than the real-real baseline (Table \ref{table:v0-v14-improvement}). This implies that the initial ACF RMSE measurements over-smoothed temporal dynamics, when in reality they may be more erratic. 

The most impactful improvement from v0 to v14 is the decrease in TSTR Mean (across features) from 1.096 in v0 to 0.588 in v14 (Table \ref{table:v0-v14-improvement}). The v14 trial was 86.39\% more accurate than v0 when it came to using synthetic data to predict the next real data step. With further refinement it may be possible to reach a point where synthetic data serves as a valid proxy for real data for downstream machine learning applications.

The v14 test had the lowest KNN asymmetry of all tests of -2.409, though still far from the real-real baseline (Table \ref{table:v0-v14-improvement}). As asymmetry moves closer to zero that means that the synthetic data more uniformly covers the real data distribution and shows less of a tendency to cluster together. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{v14results.png}
    \caption{GDP Growth vs Unemployment Rate for v14 Synthetic vs Real Data}
    \label{fig:v14-results}
\end{figure}

Visually the v14 distribution is clustered around the mean, as it is easy for a model with exposure to a small amount of data to minimize average loss by just generating average values (see \ref{fig:v14-results}). However, throughout testing I found that with the right adjustments the model can be tuned to introduce novelty outside of average values without sacrificing realism. In terms of empirical applications, until a TSTR score comparable to the real-real baseline (see \ref{table:baseline-v0-results}) is reached the synthetic data cannot be viably used downstream. 

Overall the improvements in evaluation metrics demonstrate that adversarial techniques could viably produce synthetic data from small training sets. The combined supervised and adversarial objectives of TimeGAN show the potential of the model to capture the subtle temporal relationships between macroeconomic indicators.

\subsection{Avenue for Future Study}
I observed that adversarial learning may too heavily penalize generations that are different from the mean when trained on such small datasets. Generating synthetic data at the mean of a distribution is the easiest way to produce stable, low average loss. The efforts that I made to decrease the influence of the adversarial component of the model, such as adding noise to the discriminator and increasing gamma lead to some of the most significant improvements in model performance. Therefore in future research I would like to compare the performance of TimeGAN on small datasets to non-adversarial models.

For example, a framework like variational autoencoders (VAEs) could be applied to synthetic time series generation. VAEs are probabilistic models which learn a latent probability distribution and could then use that to generate a synthetic latent distribution which is then decoded into a synthetic dataset. It is somewhat the opposite of TimeGAN, which takes in a random distribution and attempts to generate it into a realistic synthetic distribution.  

I would also like to apply my research to generating data from initial conditions. Macroeconomic data and decision making is based around the expected future movements of macroeconomic indicators. In my case, random macroeconomic datasets may have better downstream applications than a specific prediction based on initial conditions, but overall are less likely to be applicable to actual decision making. My biggest concern is that you would need substantial training data to learn patterns from each different initial condition.

\section{Ethical Considerations}
In a hypothetical scenario where synthetic macroeconomic data was to become widely used by economists/bankers, the primary ethical concern with my project becomes the question of who actually benefits from improved economic simulation, and who may be unintentionally excluded from those benefits.

\textcite{ravallion2021macroeconomicmisery} point out how changes in macroeconomic conditions have disproportionately negative impacts on the poorest Americans. Unemployment or inflation rates for example, may negatively impact the \textit{value} of the wealth of upper class Americans, but won’t actually impact their livelihood \textcite{ravallion2021macroeconomicmisery}. The poor however, are placed under immense burden from macroeconomic changes.

The major concern is that improved macroeconomic modelling only serves to benefit those with the financial literacy and wealth to take advantage of it–particularly banks, policymakers, and wealth investors. As such, macroeconomic modelling is likely to be used to benefit these upper-class groups, rather than improve the financial standing and financial literacy of the lower class.

In a hypothetical world where synthetic data is widely adopted there is the risk that synthetic data is used by a powerful actor without disclosure of where that data came from. Since macroeconomic factors are impactful to all, anybody who is in a policy making position must be transparent and accountable for the data they use. The general public, and even data-scientists, may not know exactly what patterns an algorithm is learning in training. There is the potential that synthetic data abstracts real world conditions, creating incomplete or untrustworthy datasets that are then used in decision making. 

The use of algorithmic decision making in credit-scoring and loan rating is controversial, as it obscures the process behind making very impactful decisions on people's lives. The use of synthetic macroeconomic data for decision making poses the same risks–obscuring factors in decisions which impact people's lives. If any bias is present in the dataset that is used to produce the synthetic data, then the bias patterns will be present in the synthetic data as well. When it comes to real people's lives, using synthetic data becomes a real liability.

\section{Conclusion and Future Work}
I was able to make notable improvements in the performance of the TimeGAN model on generating synthetic data from a small input dataset. Both the statistical and TSTR scores improved from my v0 to v14 tests. However, I was unable to fully prevent overfitting to the input data and the tendency to collapse toward the mean. There is an inherent conflict between generating novel, varied distributions, and loss functions which aim to minimize difference. I was able to refine this tradeoff to an extent, and produce synthetic data in v14 which demonstrated both better novelty, predictive, and distributional scores than in v0. Ultimately though I would conclude that in its current state, the size of the training dataset is too severe a limiting factor in the ability to produce temporally realistic and distributionally viable synthetic datasets using TimeGAN.

If a model were to reach the goal of producing realistic synthetic macroeconomic data to the extent that said synthetic data was indistinguishable from real, then this raises a number of ethical concerns. Aside from the obvious like fabricating data, if applied to macroeconomic modelling techniques there is a risk of further exacerbating the wealth divide or people being harmed by bias synthetic data produced from bias training data. Transparency on the generation process of synthetic data is vital to mitigating these risks.

% Defer appendix material until after the bibliography so the bibliography
% appears as part of the main paper and the following sections become Appendix.
\AtEndDocument{%
    \clearpage
    \appendix
    \section{Replication Instructions}
    \subsection{Software and Versioning}
    The codebase for this project was built in using the VSCode IDE, with all programming aspects of the program done in Python version 3.13.3. The programs were run on a Mac with an M2 Pro chip and 16G RAM. The global data is stored in csv files in the data and datag2 folders. Notably in TimeGAN, TensorFlow uses the newest Keras V3 version rather than the older version used in the original TimeGAN implementation. The difference is that Keras V3 has different session management, so you have to ensure to disable eager execution in order to run the program in the same manner as the original (use tf.compat.v1.disable\_eager\_execution() at the top of the file). With the Keras V3 version the sessions are instantiated and run slightly differently, and if there was a future update to TensorFlow versioning then you would need to either explicitly set the version back to Keras V3 or adapt to the new version.

    As far as evaluation metrics and mathematical components are concerned, these all use numpy, pandas and SciKit libraries which are more static and less frequently changed than TensorFlow versioning. Still in future implementation pay attention to where these libraries are called as they may have deprecated features that need to be adapted. All libraries and versioning requirements are stored in the requirements.txt file in the repository

    \subsection{Run Instructions}

    \begin{enumerate}
        \item \textbf{Prepare and clean the dataset.}
        \begin{itemize}
            \item Ensure the raw macroeconomic dataset is correctly structured before passing it from \texttt{main.py} into \texttt{timegan.py}.
            \item Although the initial goal was universal dataset compatibility, additional datasets require re-implementing:
            \begin{itemize}
            \item \texttt{data\_clean.py}
            \item \texttt{prep\_windows.py}
            \end{itemize}
            to match file naming conventions and dataset dimensions.
            \item The provided macroeconomic dataset works without modification.
            \item The \texttt{g2} cleaning and window scripts serve as examples for adapting the pipeline to alternative datasets.
        \end{itemize}
        \item \textbf{Clean raw CSV into per-country files)}
        \begin{itemize}
            \item File: \texttt{data\_clean.py}
            \item Arguments: 
            \begin{itemize}
                \item \texttt{--input}: Path to the raw dataset  
                (default: \texttt{data/MacroData.csv})
                \item \texttt{--outdir}: Output folder for cleaned \texttt{Country*.csv} files  
                (default: \texttt{data/clean})
            \end{itemize}
            \item Example Usage:\\
            \texttt{\# Run with defaults} \\
            \texttt{python3 data\_clean.py} \\[6pt]

            \texttt{\# Specify a different input file} \\
            \texttt{python3 data\_clean.py --input data/alt/MacroData\_alt.csv} \\[6pt]

            \texttt{\# Specify a different output directory} \\
            \texttt{python3 data\_clean.py --outdir data/clean\_custom} \\[6pt]

            \texttt{\# Specify both} \\
            \texttt{python3 data\_clean.py --input raw/newdata.csv --outdir data/newclean} \\
        \end{itemize}

        \item \textbf{Create Sliding Windows}
        \begin{itemize}
            \item File: \texttt{prep\_windows.py}
            \item Arguments:
            \begin{itemize}
                \item \texttt{--data\_dir}: Directory containing cleaned \texttt{Country*.csv} files (default: \texttt{data/clean})
                \item \texttt{--L}: Sliding window length (default: 24)
                \item \texttt{--stride}: Step size between windows (default: 1)
                \item \texttt{--val\_countries}: Validation countries (default: \texttt{Country7})
                \item \texttt{--test\_countries}: Test countries (default: \texttt{Country8 Country9})
            \end{itemize}
            \item Example Usage:\\
            \texttt{\# Run with defaults} \\
            \texttt{python3 prep\_windows.py} \\[6pt]

            \texttt{\# Change window length} \\
            \texttt{python3 prep\_windows.py --L 36} \\[6pt]

            \texttt{\# Specify custom countries} \\
            \texttt{python3 prep\_windows.py --val\_countries Country5 Country6 --test\_countries Country7 Country8} \\[6pt]

            \texttt{\# Use a different cleaned dataset folder} \\
            \texttt{python3 prep\_windows.py --data\_dir data/clean\_g2} \\[6pt]

            \texttt{\# Full custom example} \\
            \texttt{python3 prep\_windows.py} \\
            \texttt{\ \ --data\_dir data/clean\_g2} \\
            \texttt{\ \ --L 36} \\
            \texttt{\ \ --stride 2} \\
            \texttt{\ \ --val\_countries Brazil} \\
            \texttt{\ \ --test\_countries India China} \\
        \end{itemize}

        \item \textbf{Run a TimeGAN Model Version}

        \begin{itemize}
            \item File: \texttt{main.py}
            \item Run a particular experiment version with: 
            \begin{itemize}
                \item \texttt{python3 main.py X}
                \item \texttt{X} is the version identifier (e.g., \texttt{0}, \texttt{3}, \texttt{14})
            \end{itemize}
            \item Versions are implemented as \texttt{elif} blocks in \texttt{main.py} to prevent overwriting results
            \item Each version defines its own hyperparameter overrides
            \item To create a new version:
            \begin{itemize}
                \item Create a unique version label
                \item Add a corresponding \texttt{elif} block in \texttt{main.py}
                \item Call it through the command line as shown above
            \end{itemize}
            \item Using a new dataset requires adapting:
            \begin{itemize}
                \item filename paths
                \item input shapes
                \item windowing behavior
                \item dataset-specific normalization rules
            \end{itemize}
        \end{itemize}

        \item \textbf{Synthetic Data Output}
        \begin{itemize}
            \item Each run automatically creates: \texttt{artifacts/baseline\_vX}
            \item Synthetic sequences are saved as:
            \begin{itemize}
                \item \texttt{synthetic\_scaled.npy}
                \item \texttt{synthetic\_orig.npy}
            \end{itemize}
            \item Training, validation and test sets are also saved to the same folder for reference
        \end{itemize}

        \item \textbf{Run Evaluation Metrics}
        \begin{itemize}
            \item File: \texttt{eval\_metrics.py}
            \item Run with: \texttt{python3 eval\_metrics.py X}
            \item Results saved as a \texttt{json} to: \texttt{artifacts/baseline\_vX/results/results\_vX\_metrics.json}
        \end{itemize}

        \item \textbf{Generate Diagnostic Graphs}
        \begin{itemize}
            \item File: \texttt{graphing.py}
            \item Run with: \texttt{python3 graphing.py X}
            \item Results saved as a \texttt{png} to: \texttt{artifacts/baseline\_vX/}
            \item By default, graphs are produced for GDP growth and unemployment rate
            \item Modify \texttt{graphing.py} if using datasets with different indicator names
        \end{itemize}

    \end{enumerate}

    \section{Architecture Overview}
    \label{appendix:architecture-overview}

    \subsection{High-Level Repository Layout}
    \begin{itemize}

        \item \textbf{data/}
        \begin{itemize}
            \item \textbf{clean/}
            \begin{itemize}
                \item Per-country CSV files from the original 9-country dataset
                \item \texttt{MacroData.csv} (raw input data from World Bank WDI)
            \end{itemize}

            \item \textbf{datag2/} (experimental; not essential)
            \begin{itemize}
                \item \textbf{clean\_g2/}
                \begin{itemize}
                    \item Cleaned data for all countries globally (experimental)
                    \item \texttt{MacroData\_g2.csv}
                \end{itemize}
            \end{itemize}
        \end{itemize}

        \item \textbf{artifacts/}
        \begin{itemize}
            \item \textbf{baseline\_vX/}
            \begin{itemize}
                \item \texttt{config.json}
                \item \texttt{train\_scaled.npy} 
                \item \texttt{val\_scaled.npy}
                \item \texttt{test\_scaled.npy}
                \item \texttt{train\_orig.npy} 
                \item \texttt{val\_orig.npy} 
                \item \texttt{test\_orig.npy}
                \item \texttt{synthetic\_scaled.npy}
                \item \texttt{synthetic\_orig.npy}
                \item \textbf{results/}
                \begin{itemize}
                    \item \texttt{results\_vX\_metrics.json}
                \end{itemize}
            \end{itemize}
        \end{itemize}

        \item \texttt{data\_clean.py}
        \item \texttt{prep\_windows.py}
        \item \texttt{utils.py}
        \item \texttt{timegan.py}
        \item \texttt{main.py}
        \item \texttt{eval\_metrics.py}
        \item \texttt{graphing.py}
        \item \texttt{sanity.py}

        \item \textbf{.venv/} \hfill (Python virtual environment)
    \end{itemize}

    \subsection{Layout Explanation}
    The original TimeGAN repository was less modular than my version. I decided that for my use case I wanted to break each file out into a clear and distinct purpose, which allows the program to run in a modular way, so if you want to adapt to new data uses you can do so file by file. The artifacts folder contains all data results, with both the training/synthetic data saved in each file as well as the evaluation metrics and optionally graphics if you choose to run graphing.py on the trial number. Everything is based around trial numbers, and in main I chose to store trial numbers as elif statements because initially I was running into issues where I would change parameters and then accidentally overwrite results with different parameter runs. This way even if the model is re-run on the same trial number, it will use the original parameters used by that trial number and not rewrite old data. I do not think it is the most efficient and sustainable code design, but it has successfully gotten rid of the issue of rewriting trials.

    \subsection{File Descriptions}

    \begin{itemize}

        \item \textbf{Data Cleaning: \texttt{data\_clean.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes a raw macroeconomic CSV file (e.g., \texttt{MacroData.csv}) and outputs a set of per-country cleaned CSV files in \texttt{data/clean/}.  
            This prepares the dataset for sliding-window generation and later model training.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Load the raw CSV into a Pandas dataframe.
                \item Normalize, pivot, and restructure columns (e.g., indicator/year formatting).
                \item Remove NaN values and ensure consistent ordering across years.
                \item Split the full dataset into individual \texttt{CountryX.csv} files.
                \item \texttt{clean\_datag2():} performs the same process on the larger G2 dataset with different column naming conventions (experimental extension).
            \end{itemize}
        \end{itemize}


        \item \textbf{Window Preparation: \texttt{prep\_windows.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes cleaned per-country CSV files and outputs NumPy arrays of sliding windows, stored as \texttt{*.npy} files in each \texttt{baseline\_vX/} folder.  
            This produces the \texttt{[N, L, D]} training arrays used by TimeGAN.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Load cleaned country CSVs into Pandas dataframes.
                \item Convert each country’s time series into overlapping windows of length \texttt{L}.
                \item Scale features to the range \texttt{[-1, 1]} to standardize magnitude.
                \item Output summary dictionaries and NumPy arrays containing training, validation, and test window sets.
                \item \texttt{prepare\_windows\_global():} global variant used for the G2 dataset (not part of core experiment).
            \end{itemize}
        \end{itemize}


        \item \textbf{Utilities: \texttt{utils.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Provides helper functions and default model parameters.  
            Takes simple Python objects (dicts, lists, scalars) and returns initialized TensorFlow components, RNN cells, optimizers, and parameter dictionaries.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Set NumPy and TensorFlow seeds for reproducibility.
                \item Implement Xavier initialization and Adam optimizer setup.
                \item Store the base model configuration dictionary.
                \item Construct RNN cells (either \texttt{lstm} or \texttt{gru}) and stack layers.
                \item Generate utility functions used throughout model construction and training.
            \end{itemize}
        \end{itemize}


        \item \textbf{TimeGAN Model Implementation: \texttt{timegan.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes preprocessed training windows (\texttt{NumPy} arrays) and training parameters (dict), and outputs trained TensorFlow models and synthetic window samples.  
            Also stores the computation graph and handles for running networks.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Define Embedder, Recovery, Generator, Supervisor, and Discriminator networks.
                \item Build and store TensorFlow computation graph components.
                \item Implement all loss functions (AE, supervised, adversarial, moment losses).
                \item Perform learning using the Adam optimizer.
                \item Return a handles dictionary used to execute forward passes during synthetic data generation.
            \end{itemize}
        \end{itemize}


        \item \textbf{Training Orchestration: \texttt{main.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes cleaned window arrays and version-specific hyperparameter overrides, executes AE and GAN training loops, and outputs real and synthetic NumPy arrays into each \texttt{baseline\_vX/} directory.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Load and assemble windowed data from \texttt{prep\_windows.py}.
                \item Extract array shapes and initialize the full TimeGAN architecture.
                \item Run the Autoencoder warmup stage then GAN training stages.
                \item Save \texttt{train\_orig.npy}, \texttt{test\_orig.npy}, \texttt{synthetic\_orig.npy}, and their scaled counterparts.
            \end{itemize}
        \end{itemize}


        \item \textbf{Evaluation Metrics: \texttt{eval\_metrics.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes real and synthetic NumPy arrays from a \texttt{baseline\_vX/} directory and outputs a JSON file of metric results.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Load \texttt{*.npy} arrays for real and synthetic data.
                \item Compute all evaluation metrics (KS Max, WD Mean/Max, Frobenius norm, ACF RMSE, TSTR, KNN symmetry metrics).
                \item Store results in \texttt{results\_vX\_metrics.json}.
            \end{itemize}
        \end{itemize}


        \item \textbf{Visualization: \texttt{graphing.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            Takes synthetic and real NumPy arrays for a version \texttt{X} and outputs PNG visualizations in the corresponding \texttt{baseline\_vX/} folder.

            \item \textbf{Key Uses:}
            \begin{itemize}
                \item Load \texttt{train\_orig.npy} and \texttt{synthetic\_orig.npy}.
                \item Plot GDP vs.\ Unemployment scatter distributions.
                \item Plot GDP and Unemployment time series.
                \item Save plots as PNG files for qualitative inspection.
            \end{itemize}
        \end{itemize}


        \item \textbf{Sanity Checks: \texttt{sanity.py}}
        \begin{itemize}
            \item \textbf{Purpose:}
            A small diagnostic script used to ensure version isolation and prevent accidental overwriting.  
            Takes no formal input other than existing version folders and prints debugging information; not required for standard usage.
        \end{itemize}

    \end{itemize}


    \subsection{Default Training Parameters}

    \begin{table}[h]
        \centering
        \caption{Default training parameters used in the baseline TimeGAN implementation.}
        \label{table:default-params}
        \begin{tabular}{ll}
        \toprule
        \textbf{Model Parameter} & \textbf{Default Value} \\
        \midrule
        $L$ (sequence length)     & 24 \\
        $D$ (feature dimension)   & 4 \\
        $z_{\text{dim}}$          & 4 \\
        batch\_size               & 64 \\
        ae\_warmup\_it            & 600 \\
        gan\_iters                & 3000 \\
        $\gamma$                  & 1.0 \\
        learning\_rate            & 0.001 \\
        module                    & \texttt{"gru"} \\
        hidden\_dim               & 24 \\
        num\_layers               & 2 \\
        \bottomrule
        \end{tabular}
    \end{table}

    These are the training parameters passed into the model during training. They are also the primary hyperparameters varied during experimentation.

    \begin{itemize}
        \item \textbf{$L$}: Length of each sliding window (number of time steps). In this project, $L = 24$ corresponds to a 24-year sequence.
        
        \item \textbf{$D$}: Number of dimensions in the training data (number of economic indicators). This was fixed at $D = 4$ for GDP growth, inflation rate, unemployment rate, and population growth.
        
        \item \textbf{$z_dim$}: Dimensionality of the random noise vectors fed into the generator, controlling generative diversity.
        
        \item \textbf{batch\_size}: Number of sliding windows passed to the model during each training step.
        
        \item \textit{ae\_warmup\_it}: Number of autoencoder warmup iterations prior to adversarial training.
        
        \item \textit{gan\_iters}: Number of full GAN training iterations.
        
        \item \textbf{$\gamma$}: Weighting coefficient balancing the supervised loss and adversarial loss components within the generator loss.
        
        \item \textbf{learning\_rate}: The step size used during optimization; controls how quickly model weights update during backpropagation.
        
        \item \textit{module}: The type of recurrent cell used in the model’s networks. Supported options include:
        \begin{itemize}
            \item \texttt{``gru''}: Gated Recurrent Unit; uses an update gate (memory retention) and reset gate (context forgetting). More lightweight and computationally efficient.
            \item \texttt{``lstm''}: Long Short-Term Memory; uses three gates and maintains a cell state, offering greater robustness at higher computational cost.
        \end{itemize}
        
        \item \textit{hidden\_dim}: Size of the hidden state in each neural network layer; determines the number of latent features used to encode temporal structure.
        
        \item \textit{num\_layers}: Number of stacked layers in the recurrent neural network architecture.
    \end{itemize}
    
    \section{Additional Results and Exploration}
    \subsection{Significant Trials and Results}
    \label{appendix:further-results}

    In each different trial iteration I changed a different parameter of the model, and in some cases I made slight changes to the training process such as adding noise to the discriminator, or changing RNN cell type. Some of the most notable version changes are shown in the table below. The bolded cells represent the best performance of that evaluation metric across the sample experiments.

    \begin{table}[h]
        \centering
        \caption{Intermediate model evaluation metrics across selected development versions (v0, v3, v4, v8, v10).}
        \label{table:appendix-intermediate-results}
        {\small
        \setlength{\tabcolsep}{6pt}
        \renewcommand{\arraystretch}{1.05}
        \begin{tabular}{l c c c c c}
            \toprule
            \textbf{Metric} & \textbf{v0} & \textbf{v3} & \textbf{v4} & \textbf{v8} & \textbf{v10} \\
            \midrule
            KS Max            & 0.975 & 0.969 & 0.951 & 0.976 & 0.975 \\
            WD Mean           & 0.206 & 0.189 & 0.176 & 0.214 & 0.181 \\
            WD Max            & 0.384 & 0.319 & 0.289 & 0.377 & 0.346 \\
            Frob Diff         & 3.432 & 2.304 & 3.479 & 2.969 & 2.909 \\
            ACF RMSE          & 0.155 & 0.278 & 0.132 & 0.147 & 0.225 \\
            TSTR Mean         & 1.096 & 3.554 & 1.559 & 1.246 & 0.251 \\
            KNN Mean S--R     & 12.728 & 12.302 & 11.082 & 13.209 & 12.328 \\
            KNN Mean R--S     & 15.755 & 15.929 & 15.955 & 16.291 & 15.486 \\
            KNN Asymmetry     & -3.144 & -3.627 & -4.873 & -3.018 & -3.157 \\
            \bottomrule
        \end{tabular}
        }
    \end{table}

    In v3 I increased the iterations of the autoencoder warmup from 600 to 1000. The goal was to allow the model more time to learn latent encoding and transformations before the adversarial component was trained. This effect showed very clearly in the Frobenius difference, with a significant improvement in cross-feature correlations. Extending the autoencoder warmup iterations also showed improvements from v0 in distributional metrics (WD) (\ref{table:appendix-intermediate-results}). Although this came at the expense of ACF and TSTR performance, for early testing I thought that the improvement in correlation matrices was worth keeping this change in as a point to further build from.

    In v4 I decreased the learning rate from 0.001 to 0.0001, which led to substantial improvement in distributional similarity and the best autocorrelation score across all tests (\ref{table:appendix-intermediate-results}). The goal was to reduce overfitting on the small dataset by decreasing the rate at which the model learns. The TSTR score was significantly lower from v3 to v4, which indicates that the actual downstream applicability of this version was far better than the previous, and back toward the baseline. In these early tests everything came almost as a direct tradeoff. In the areas where v4 showed improvement from v3, it also lost some of the improvements that v3 made over v0. Notably though, v4 had much smoother loss gradients in the supervised and autoencoder loss, which was a large part of the reason I was inclined to keep it in. The early iterations were largely about improving model health, observed primarily from loss values.

    Between v4 and v8 progress stagnated, but in v8 I had somewhat of a breakthrough by adding Gaussian noise with a standard distribution of 0.02 to the input latent sequences. This served to add some additional randomness to the discriminator; a standard stabilization technique in GANs. It helps to prevent the discriminator from becoming too confident too early and slows down its convergence to give time for the generator to learn. With no noise, the discriminator quickly overfits to small differences in the latent space, but the added uncertainty in the discriminator loss helps the generator to better learn more erratic latent transitions. The result was improvements from v0 in Frob Diff, ACF RMSE, and KNN asymmetry, indicating that the model better represented intra-feature dependence, temporal patterns, and better filled the real distribution.

    Interpreting loss values is tricky, because you do want them to converge toward 0 throughout training, but if they ever reach 0 then it is evidence of overfitting, and if they converge too low too quickly then it is evidence of collapse. With the autoencoder, lower loss is not necessarily correlated with better generations. For v10 I halved autoencoder loss in the actual model implementation. This helped decrease the extent that the autoencoder constrained latent space and gave the generator more freedom to generate synthetic data, as until this point all results tended to be repetitive and low-variance. Keeping autoencoder iterations high from v3 allows for convergence to stable loss values, while halving the loss ensures less pull toward exact reconstruction. The result of v10 was a test that was across the board similar to or better than all metrics in v0, but with a significant improvement in one of the most important metrics: TSTR mean. With a v10 score of 0.251, and every other iteration having a TSTR score of over 1, this was a clear and very significant improvement.

    \subsection{Additional Meaningful Changes}
    There were a few other changes that did not lead to notable changes in evaluative performance, but were observed to improve loss values into a more healthy range. The first was reducing GAN iterations from 3000 to 2000, a change done in v1 that was kept through to the final version v14. In v0, the loss values plummeted (see \ref{table:loss-components}), indicating clear collapse of loss values and overfitting of the model. In v1 the loss values were only slightly better, but visually looking at the gradients of the loss function (printed every 100 iterations in training), it was clear that the final 1000 iterations (from 2000-3000) was the primary area where the collapse was happening.

    \begin{table}[h]
        \centering
        \caption{Generator, Discriminator, and Supervised Losses Across Model Versions.}
        \label{table:loss-components}
        {\small
        \setlength{\tabcolsep}{10pt}
        \renewcommand{\arraystretch}{1.05}
        \begin{tabular}{l c c c}
            \toprule
            \textbf{Version} & $\mathcal{L}_{D}$ & $\mathcal{L}_{G}$ & $\mathcal{L}_{S}$ \\
            \midrule
            v0  & 0.0000 & 0.0004 & 0.0003 \\
            v1  & 0.0001 & 0.0006 & 0.0006 \\
            v14 & 0.0015 & 0.0019 & 0.0003 \\
            \bottomrule
        \end{tabular}
        }
    \end{table}

    Part of the improvement in loss values observed between versions v1 and v14 was due to the raising of $\gamma$ from $1$ to $5$ in v2. $\gamma$ is just the weighting of supervisor loss relative to the adversarial loss in the generator. By increasing $\gamma$, the generator learned to place more emphasis on latent temporal transitions, and less emphasis on adversarial objectives, which helped reduce the collapse of loss values and overfitting of the model.

    In v13 I increased the $z_dim$ from 4 to 16. As the model learns from latent reconstructions, adding more dimensions to the noise vectors input to the generator allows for the model to learn more complex, refined relationships (see \ref{table:default-params}). Though the evaluation metrics nor loss values did not change significantly from the introduction of this change in v13, I decided to leave the higher dimensionality of noise vectors in the v14 version, which may have contributed to its ability to balance latent representations, temporal relationships, and novelty.

    \subsection{Dataset Size and g2 Test Results}
    Out of curiosity, I wanted to test the full extent of the limitations of a small dataset. I re-gathered the same macroeconomic datapoints from the World Bank WDI (\cite{worldbank_wdi_2024}), but this time for the set of all countries in the world. This global dataset had about 717 unique training windows (many countries were omitted during the data cleaning process due to lack of data points). On the larger dataset the model's performance surprisingly varied widely, and was significantly less accurate in every way than the small training set. As you can see from the graphs of unemployment with time (\ref{fig:unemp-g2}) and gdp growth per time step (\ref{fig:gdp-g2}), the synthetic data produced by the global model clustered toward extremes. While the tuned model performed clearly better on the small dataset, I would like to do further research into what specifically made the model with the larger training set produce such anomalous results. My inclination is that the adjustments which lead to optimization on a small dataset may cause either collapse or explosion of loss values on a larger dataset, leading to the model overemphasising the importance of outliers in the original data.

    \begin{figure}[h]
        \centering

        \begin{minipage}[t]{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{unempg2.png}
            \caption{Unemployment Over Time (g2 Synthetic)}
            \label{fig:unemp-g2}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{gdpg2.png}
            \caption{GDP Growth Over Time (g2 Synthetic)}
            \label{fig:gdp-g2}
        \end{minipage}

    \end{figure}

    I also think that the amount of sequence overlap in sliding windows–a decision which was necessary for increasing the training steps availability on my small dataset–was ultimately harmful to the global dataset. When there are enough original batches for training without implementing sliding windows, then the inclusion of sliding windows just means that trends will be repeated and perhaps overemphasized in training. Since my original countries were chosen for relative stability in their economies while still representing different geographic areas, it makes sense that there were not many extreme values present in the training data. Using all countries in the world however would expose the model to training data from developing economies which are more susceptible to shocks or high inflation. This could be another reason why extreme points are more prevalent in the synthetic data produced by the global dataset, as the global model would be exposed to many sequences with such inconsistent values.

}

\printbibliography

\clearpage

\onecolumn

\end{document}