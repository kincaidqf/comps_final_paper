\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% Use biblatex (default backend=biber). A temporary change to bibtex was used
% as a fallback during debugging and is now reverted.
\usepackage[style=numeric,sorting=nyt]{biblatex}
\addbibresource{references.bib}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series)
    /Author (Kincaid Fries)
}

% set the title and author information
\title{TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series}
\author{Kincaid Fries}
\affiliation{Occidental College}
\email{kfries@oxy.edu}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Background}
The use of machine learning in financial prediction has grown rapidly due to its ability to model complex, multivariable relationships and generate realistic predictions. Machine learning is particularly effective in equities markets where thousands of points of training data are available daily for stock movements. In contrast, macroeconomic data takes months of study, surveying, and research to come up with even a single realistic measurement. Consequently, macroeconomic forecasting models rely on quarterly historical data at best. \textcite{baltazar2020sustainableeconomies} note that small datasets affect the reliability of macroeconomic modelling. Other challenges like changes in calculation methods, unavailability of data for certain time periods, and changes in indicator definitions make it hard to train macroeconomic models without overfitting to small datasets. Recent advances in synthetic data generation offer a promising alternative, allowing researchers to create realistic simulations of economic behavior without relying solely on historical data.

\subsection{Goal}
The goal of my senior comprehensive project is to evaluate whether a machine learning model trained on small macroeconomic datasets can be used to generate realistic synthetic time series. By normalizing macroeconomic indicators across different economies, I hope to address the issue of relatively limited time-series for a single country by taking training data from multiple countries. My project will explore whether synthetic time-series produced by a model trained on data from multiple countries could potentially address the problem of lack of data availability, and serve as a valid counterpart to real economic data in forecasting applications.
Abstracting from this project, data availability is becoming an increasingly prevalent issue as the technological world embraces machine learning. Major data companies like Meta and OpenAI have faced lawsuits over data piracy, and smaller companies want to utilize the power of machine learning to drive decision making, but may face issues with security or compliance that prevent them from training models. If we had the power to take a small dataset and extend it, or create realistic alternative datasets using synthetic data generation, then a number of these problems could be solved. Security, privacy, and copyright could be better preserved while still having access to the quantity of data needed to train machine learning models. 

\subsection{Approach}
I will implement a TimeGAN model, which combines a supervised autoencoder to learn temporal dynamics with a generative adversarial network (GAN) to generate realistic synthetic data \textcite{yoon2019timeseriesgenerative}. The model will be trained using macroeconomic data taken from developed, free-market economies, to ensure there is consistency in the underlying macroeconomic trends it identifies. To evaluate whether the generated synthetic data is realistic, I will compare the properties of the synthetic data with real world data using a number of statistical tests focused on distributional similarity, cross time similarity, and novelty. Additionally, I will examine the performance of a forecasting model using the train-synthetic test-real (TSTR) method–training a simple forecasting model on the synthetic data to predict the next step in the real data. Through TSTR evaluation I will be able to assess the potential viability of using synthetic data in training models for real data tasks.
By looking at both statistical factors and forecasting model performance, I hope to have a comprehensive framework for assessing the viability of the synthetic data. I will then iterate over the model, adjusting training parameters in order to see what areas are particularly effective for model optimization on small datasets.

\section{Technical Background}
\subsection{Macroeconomic Indicators}
Macroeconomic indicators are data points which quantify the health of an entire economic market, usually at a country-wide level. They are impactful across all levels of society and business, and are even politically salient, as \textcite{ravallion2021macroeconomicmisery} notes the impact of macroeconomic indicators on electoral outcomes. The macroeconomic indicators that I will focus on will be gross domestic product growth (GDP), inflation rate (I), unemployment rate (R), and population growth (P), which were selected with insight from the research of  \textcite{ravallion2021macroeconomicmisery} and \textcite{baltazar2020sustainableeconomies}. Notably, population growth is not typically considered a macroeconomic indicator, but it helps control for unexplained factors in the other indicators. For example, if GDP and unemployment rates both go up, a possible explanation would be there was a substantial increase in population.

\subsubsection{Selected Indicators}
\begin{description}
    \item[\textbf{GDP}] measures the entire monetary value of an economy, including all the goods and services produced by it and its constituents, its trade with other countries, and even the value of the intellectual property in the country. I will be using the rate of GDP growth expressed as a percentage change in GDP for this project.
    \item[\textbf{Inflation Rate}] is the percent change in price of a pre-defined theoretical ‘basket’ of goods. The ‘basket’ is a collection of common household goods which every individual regardless of income level will regularly purchase.  
    \item[\textbf{Unemployment Rate}] is the proportion of the unemployed labor force actively seeking work to the total number of people in the labor market. A high unemployment rate means there are a significant number of individuals without income seeking employment. The efficient unemployment rate is typically around 5\% \cite{voxeuUnemployment}.
    \item[\textbf{Population Growth}] is the percent change in the population of study, measured annually.
\end{description}

\subsubsection{Relevance of Macro-Indicators}
\textcite{ravallion2021macroeconomicmisery} discusses how macroeconomic indicators must be viewed in conjunction with each other, as no individual measure fully captures the state of the economy. When inflation is high, there is usually excess spending in the economy, which increases the amount of goods and services being purchased, which in turn increases GDP. The heightened demand can lead to increased hiring and lower unemployment. Whenever one of the indicators changes, it will surely have an impact on the other indicators and the overall state of the economy. This project will look at each of the indicators outlined above in order to capture a holistic picture of the economy. Macroeconomic forecasting is used by banks, the federal government, and a variety of institutions both in and out of the economic industry. For example, \textcite{baltazar2020sustainableeconomies} detail how macroeconomic models can be used by banks to find their best response to economic shocks, potentially helping avoid situations like the 2008 financial crisis.

\subsection{Relevent Machine Learning Terminology}
\begin{description}
    \item[\textit{Supervised learning}] uses labelled data and produces consistent outputs for given input. While effective for forecasting, the labelling is demanding and it is not well suited for maintaining temporal dynamics in time series generation \cite{yoon2019timeseriesgenerative}.
    \item[\textit{Unsupervised learning}] identifies patterns in unlabelled data. Due to not requiring the labelling step, it is easier to implement than supervised learning, but it can be a high compute ‘black box’ where the internal steps aren’t understood.
    \item[\textit{Recurrant Neural Networks}] are an artificial neural networks that are more suited for processing sequential data. Traditional neural networks pass data between nodes without ‘remembering’ the previous node. RNNs pass data with an additional parameter allowing nodes to observe both the current and previous state in order to learn temporal dynamics.
    \item[\textit{Generative Adversarial Networks (GANs)}] consist of a generator neural network competing with a discriminator to produce synthetic data that the discriminator can’t detect \cite{awsGAN}. As the generative network gets better at generating synthetic data, the discriminator network will become worse at determining the difference between real and synthetic data.
    \item[\textit{Latent Space}] refers to the compression of input data into a lower dimensional space that preserves essential features of the data’s structure \cite{ibmLatentSpace}.
    \item[\textit{Autoencoders}] are the primary neural networks that learn to encode data to a latent space and then reconstruct it, allowing them to learn the structure of the data in the process \cite{ibmLatentSpace}.
\end{description}

\subsection{Choosing TimeGAN}
\label{sec:choosing_timegan}
TimeGAN stands for \textbf{Time-Series Generative Adversarial Network}, a proprietary machine learning model created by \textcite{yoon2019timeseriesgenerative} combining supervised, unsupervised, and adversarial learning to encourage the model to follow the temporal dynamics of training data. The goal of their research stems from the inadequacy of other machine learning models to generate synthetic data that accurately takes temporal correlations into account \cite{yoon2019timeseriesgenerative}. 
Unlike other GANs, TimeGAN is trained on a 3D dataset of multiple independent time-series, shaped as $[N, L, D]$, where $N$ is the number of sequences (e.g., countries), $L$ is the number of time steps, and $D$ is the number of dimensions (e.g., GDP, CPI). This makes TimeGAN perfect for my goal of training a model on multiple countries data.

\subsection{TimeGAN Architecture}
The main components of the TimeGAN architecture are pictured in Figure \ref{fig:TimeGAN-Architecture}. Notably the supervisor is not pictured, but operates in the latent space between the embedder and recovery networks. The partial derivates $\delta\mathcal{L}$ represent the loss values used to update each network during training. The training parameters (constant in each training iteration) are represented as $\theta$.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{TimeGAN training.png}
    \caption{Components of TimeGAN Training Architecture. Adapted from \textcite{yoon2019timeseriesgenerative}.}
    \label{fig:TimeGAN-Architecture}
\end{figure}

\begin{description}
    \item[\textbf{Embedding Network}] The embedding network (\textit{e} in \ref{fig:TimeGAN-Architecture}) encodes the training data into a lower-dimensional latent space while preserving the temporal relationships. After reducing the dimensionality of the real data, the model is able to learn the structure and dynamics of the data when it is retrieved from the latent space.
    \item[\textbf{Recovery Network}] The recovery network (\textit{r} in \ref{fig:TimeGAN-Architecture}) retrieves or decodes the data embedded in the latent space back into the original data-space. The combined network between the embedding and recovery networks forms the autoencoder section of the TimeGAN.
    \item[\textbf{Supervisor Network}] The supervisor network (not pictured in diagram) works entirely in the latent space learning to predict the next step hidden state from the previous. The supervisor provides its synthetic latent transitions to the Generator Network, so that rather than trying to generate the entire dynamics of the dataset from scratch, the Generator has a per-timestep guidance on how to generate data that matches the supervisors hidden transitions.
    \item[\textbf{Generator Network}] The generator network (\textit{g} in \ref{fig:TimeGAN-Architecture}) generates sequences (time-series) from an input series of a random distribution of Gaussian noise. Through competition with the discriminator and ‘model’ transitions from the supervisor, the generator learns how to better convert the random noise into a realistic distribution.
    \item[\textbf{Discriminator Network}] The discriminator network (\textit{d} in \ref{fig:TimeGAN-Architecture}) is the adversarial part of the model which attempts to distinguish between real and synthetic generations. Its observations are used to guide the generator to create more realistic output.
\end{description}

What makes this model unique is that the embedding and recovery networks together form the autoencoder to refine latent representations, while the supervisor learns time-step shifted latent transformations (the transition from latent representation at time t to time t+1). The autoencoder refined and supervisor time-shifted, latent patterns help the generator to create realistic data for the next time step based on the current time step. The adversarial component of the discriminator provides further reinforcement on generating quality synthetic sequences. The TimeGAN performs these functions described above iteratively, allowing it to improve how it encodes features and generates representations \cite{yoon2019timeseriesgenerative}. The combination of GAN and autoencoder architecture make a TimeGAN the perfect model for my project: optimized to generate realistic time series data.

\subsection{Loss Functions}
\subsubsection{Autoencoder (reconstruction) Loss}
The autoencoder uses mean squared error; a standard loss function in machine learning. The result is a quantification of the average difference between the input into the embedder and output from the recovery network, allowing the autoencoder portion of the model to learn better how to encode and recover data from the latent space.

\begin{equation}
    \mathcal{L}_{\text{AE}} = \frac{1}{L-1}\sum_{t=0}^{L-1}\mathbb{E}\left[\left\|X_t - \hat{X}_t\right\|^2\right]
\end{equation}

First the function takes the squared absolute value of the difference between \(X_t\) and \(\hat{X}_t\), where \(X_t\) is the real data fed into the embedder at time step \(t\), and \(\hat{X}_t\) is the data recovered from the latent embedding by the recovery network. The sum of squared error terms is averaged over the number of time steps $L$ (the length of the series).

\subsubsection{Supervised Loss}
The supervised loss is also a mean squared error computed over latent representations shifted by one time step. Here, $\hat{H}_{t+1}$ denotes the supervisor network's prediction of the latent state at time $t+1$, and $H_{t+1}$ denotes the true latent state at time $t+1$. The supervised loss therefore encourages the model to learn realistic temporal transitions in the latent space.

\begin{equation}
    \mathcal{L}_{\text{Sup}} = \frac{1}{L-1}\sum_{t=0}^{L-2}\mathbb{E}\left[\left\|H_{t+1} - \hat{H}_{t+1}\right\|^2\right]
\end{equation}

\subsubsection{Generator Loss}
The generator loss has two components. First there is the adversarial loss, which is the component that comes from the discriminators ability to correctly identify the real data from the synthetic. Second is the supervised loss, which is weighted by a gamma coefficient that simply represents the amount of emphasis placed on supervised versus adversarial loss. The loss function is as follows.

\begin{equation}
        \mathcal{L}_{\text{G}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right] + \gamma \mathcal{L}_{\text{Sup}}
\end{equation}

This loss function first takes a logit, which is the raw unbounded output of the discriminator on the synthetic latent sequence $\hat{H}$, and passes it through the sigmoid function which compresses it into a probability from $[0,1]$. A logit is a score, not a probability, so this term can be interpreted as: $\sigma\!\big(D_{\text{logit}}(\hat{H})\big)$ = the probability of seeing a logit value of $D_{\text{logit}}(\hat{H})$. The logit to sigmoid process is done because the discriminator does not output raw probabilities, and internally it makes the derivatives of the loss function much easier to derive. The negative coefficient is due to the fact that loss functions always aim to penalize error rather than reward success. Since the generator is trying to minimize the probability that its synthetic data is labelled synthetic by the generator, the adversarial element of this loss function is negative.

\subsubsection{Discriminator Loss}
The discriminator loss function uses a standard function for yes no classification called binary cross-entropy. Binary cross-entropy is made up of two components, the probability score of assigning fake data the label of fake, and the probability score of assigning real data the label of fake. 

\begin{equation}
    \mathcal{L}_{\text{D}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(H\right)\right)\right)\right] - \mathbb{E}\left[\log\left(1 - \sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right]
\end{equation}

Both the supervisor and discriminator use expectation notation $\mathbb{E}[\cdot]$, which denotes the integral over the data probability distribution, e.g.\ $\mathbb{E}_{x\sim p(x)}[f(x)]=\int f(x)\,p(x)\,dx$, and in practice is approximated by a finite-sample average over a mini-batch: $\mathbb{E}_{x\sim p(x)}[f(x)]\approx\frac{1}{m}\sum_{i=1}^m f(x_i)$, where $m$ is the mini-batch size and $x_i$ are the samples.

\section{Prior Work}

First and foremost, my project relies heavily on the original TimeGAN implementation by \textcite{yoon2019timeseriesgenerative}. They identify that on their own, GANs “do not adequately attend to the temporal correlations unique to time-series data”, while purely supervised models are “inherently deterministic” (\textcite{yoon2019timeseriesgenerative}, 1). Their proposal for addressing this disparity was to create an interwoven framework that provides both the benefits of supervised models and the adversarial component of GANs. However, \textcite{yoon2019timeseriesgenerative} are not the only researchers who aim to optimize the performance of a GAN across temporal features. The unique aspect of their project that stood out to me was the fact that the model is trained on batches of data which is ideal for training the model on macroeconomic conditions using multiple different countries for input. 

Aside from the original TimeGAN repository, my project draws inspiration from studies that reflect the technical, theoretical, and evaluative aspects of synthetic data generation and machine learning applications in economics. The first applies GANs to generating synthetic financial scenarios (\textcite{rizzato2023generativeadversarial}). Another applies a macroeconomic model to loan default rate predictions (\textcite{baltazar2020sustainableeconomies}). Finally, \textcite{yuan2024multifacetedevaluation} and \textcite{livieris2024anevaluationframework} provide a reference for how to go about my evaluation metrics and results discussion.

\subsection{Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation}
\textcite{rizzato2023generativeadversarial} propose a new generative adversarial network approach to synthetic financial scenario generation, which they call Jinkou. Jinkou was made to generate realistic synthetic time series datasets on the movement of equities under different macroeconomic conditions \cite{rizzato2023generativeadversarial}. Unlike the original TimeGAN paper, they introduce a macroeconomic element which offers a similar conceptual approach and methodology to my own goals for this project. 

\subsubsection{Technical Approach}
They first took real datasets on equities with specific variables related to stock performance, and augmented that dataset with global state variables describing macroeconomic conditions \cite{rizzato2023generativeadversarial}. Their proprietary Jinkou method uses a combination of a bidirectional GAN (BiGAN) and conditional GAN (cGAN) to improve the probability distribution of the synthetic data \cite{rizzato2023generativeadversarial}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{bigan_vs_cgan.png}
    \caption{Visual comparison of BiGAN and Conditional GAN training architectures. Adapted from \textcite{rizzato2023generativeadversarial}.}
    \label{fig:bigan-vs-cgan}
\end{figure}

The BiGAN maps latent space to the data space with an autoencoder, similar to the embedder and recovery networks of the TimeGAN. The synthetic time series generated by the trained BiGAN are used as the input for the cGAN, which learns to map each datapoint \textit{y} to a probability score \textit{p} drawn from the dataset distribution of \textit{P} \cite{rizzato2023generativeadversarial}. The cGAN refines the results of the BiGAN by more accurately mapping the data to the probability distribution expected of the data. They perform statistical analysis of their synthetic data generated under four different market conditions (bull market, bear market, volatile market, and debt crisis). They evaluated the synthetic change in stock price from start to end of the time series with the real percent change in stock price \cite{rizzato2023generativeadversarial}. They found similar values for synthetic vs real data, typically with a difference between [-1,2]\%, indicating that their synthetic data was sufficiently accurate to the real data.

\subsubsection{Key Takeaway}
This project demonstrates how synthetic data is a viable way of simulating market scenarios under different macroeconomic conditions. Specific to my project, \textcite{rizzato2023generativeadversarial} found the use of GAN models to be effective for generating synthetic time-series, and were able to compare that synthetic data to real data with a satisfactory degree of accuracy. Their results validate the importance of macroeconomic indicators in financial modelling.

\subsection{Sustainable Economies: Using a Macroeconomic Model to Predict How the Default Rate is Affected Under Economic Stress Scenarios}
\textcite{baltazar2020sustainableeconomies} build a macroeconomic stress-testing framework to predict loan default rates under hypothetical, simulated macroeconomic conditions–a framework they argue would allow banks and policy makers to improve their decision making in economic downturns \textcite{baltazar2020sustainableeconomies}. They use a predefined macroeconomic model rather than machine learning, having that model generate random Monte Carlo simulations \textcite{baltazar2020sustainableeconomies}.\\

Since this is not a machine learning project, my takeaways were not technical but theoretical. They validate the importance of GDP and CPI as revealing macroeconomic indicators, and acknowledge the problem of small datasets for macroeconomic forecasting \textcite{baltazar2020sustainableeconomies}. This study confirmed the relevance and importance of everything I am working on for my project: the use of macroeconomic indicators for understanding economic conditions, the downstream effects of macroeconomic conditions on all aspects of finance and the economy, and the necessity and potential applications of synthetic macroeconomic data in augmenting existing datasets.

\subsection{A Multi-Faceted Evaluation Framework for Assessing Synthetic Data}
This study, by \textcite{yuan2024multifacetedevaluation}, proposes a novel method for the evaluation of synthetic datasets, which they call SynEval. Their evaluation method has three dimensions–fidelity, utility and privacy–of which I am interested in ‘utility’, their measure of the applicability of the synthetic data \cite{yuan2024multifacetedevaluation}.

\subsubsection{Technical Approach}
\indent{}\textcite{yuan2024multifacetedevaluation} refer to the ‘utility’ of a synthetic dataset as its ability to be used downstream for machine learning. This aspect is what I am most concerned with, as my goal is to create synthetic data that is academically and empirically useful. Downstream machine learning refers to the application of the generated data to training/testing machine learning models. The utility evaluation they proposed is based on a framework referred to as TSTR (Train-Synthetic-Test-Real) \cite{yuan2024multifacetedevaluation}. They perform a study on synthetic data generated by popular LLMs ChatGPT 3.5, Claude 3 Opus, and Llama 2 13B. In this case, they used 50 samples from a software product review dataset to train LLM’s to generate synthetic review-rating data \cite{yuan2024multifacetedevaluation}. They then trained four sentiment classification models; one on the real dataset, and the other three on the synthetic datasets produced by the LLMs \cite{yuan2024multifacetedevaluation}. The sentiment classification models were evaluated on unseen test data, with Mean Absolute Error (MAE) and percentage accuracy of correct classifications used to evaluate the performance of each model. 

\subsubsection{Key Takeaway}
In evaluating the synthetic datasets on downstream machine learning performance, they found that the model trained on synthetic data generated by Claude had a MAE of 1.2929 and accuracy of 67.68\%, which was very comparable to the model trained on real data (MAE 1.3019, accuracy 67.92\%) \cite{yuan2024multifacetedevaluation}.\textcite{yuan2024multifacetedevaluation} results prove that synthetic data can be effectively used on downstream machine learning tasks. Their methodology of using downstream machine learning as an evaluation metric for the quality of synthetic data validates my plan of using a forecasting model trained on synthetic versus real data as an evaluation metric. 

\subsection{An Evaluation Framework for Synthetic Data Generation Models}
In the research performed by \textcite{livieris2024anevaluationframework}, they compared a number of different statistical methods specifically for evaluating how realistic synthetic data is to real data. They compare data generated by five synthetic data generation models using standard tests. The three tests they used which I find most relevant to my project were Wasserstein-Cramer's V-test, novelty test, and anomaly detection test \cite{livieris2024anevaluationframework}. 

\begin{itemize}
    \item \textit{Wasserstein-Cramer's V Test}: a test that combines the Wasserstein distance–the quantification of the distance between the probability distributions of the real and synthetic data–with Cramer’s V to account for the distributions across different variables \cite{livieris2024anevaluationframework}. The test evaluates if synthetic data matches the distribution of real data accounting for shape and variance, with low scores being better. Since Wasserstein-Cramer's V Test scores are not bounded to a range, it is somewhat arbitrary determining what is a good score or not. 
    \item \textit{Novelty Test}: a measure of the synthetic data model’s ability to generate new situations not present in the original dataset. It is calculated as the ratio of unmatched instances between the synthetic dataset, where an instance is considered match if $\left| s_i - r_i \right| < \alpha$,
    with $s_i$ representing a synthetic datapoint, $r_i$ representing a real point, and $\alpha$ being some predefined threshold \cite{livieris2024anevaluationframework}. 
    \item \textit{Anomly Detection Test}: an isolation forest is trained on the real data, then applied to the synthetic dataset, to which it assigns an abnormality score to each point \cite{livieris2024anevaluationframework}. A high degree of abnormality would mean that the synthetic dataset has a number of points that do not match the distribution of the real data. An isolation forest is an unsupervised model which isolates anomalies from a dataset using a series of decision trees.
\end{itemize}
\subsubsection{Key Takeaway}
While the anomaly detection test is interesting, I think within my overall framework of evaluating distributional similarity, it is not necessary, as anomalous synthetic data will be clearly visible from other evaluation metrics. The Wasserstein Distance however, I will use as part of my distributional evaluation. Implementing the combination with Cramer’s V-test is not a commonly accepted practice and makes the results a bit less interpretable, so I will focus just on Wasserstein Distance rather than the combined test. Additionally, I will implement the novelty test through a simple nearest neighbors Euclidian distance calculation.

\section{Methods}
\subsection{Sourcing Macroeconomic Data}
For my macroeconomic training data, I used the World Bank’s World Development Indicators (WDI) online database \cite{worldbank_wdi_2024}. They have data for every country in the world going back decades, with a variety of data points normalized to the same units available for free use. From there I gathered a training dataset of the USA, Japan, Australia, UK, Canada, South Korea, Denmark, Germany and France–countries which I chose for having developed economies with reliable data going back 60+ years.

\subsection{Data Pre-Processing}
My data points of GDP growth, inflation rate, unemployment rate, and population growth were chosen deliberately to represent a snapshot of countries macroeconomic conditions without having collinearity between any of the features. GDP growth gives an idea of how the dollar quantification of the economy changes annually, while inflation rate helps control for the actual value of those dollars change adjusted for inflation. Unemployment rate along with inflation rate are considered two of the most revealing macroeconomic indicators, while population growth helps to control for some of the variation in unemployment and GDP growth.

In order to control for the absolute value differences between the different indicators and their scaling, everything was processed using a standard min-max scalar compression to [-1,1], with the range of the data being saved in order to decompress the synthetic series at the end for comparison.

The most significant pre-processing decision that I made was to divide the datasets into batches of sliding windows. With only 9 training countries, without any changes I would have had a [9, 56, 4] training dataset (see \ref{sec:choosing_timegan} for training shape). With only 9 input series of length 56, this was simply too small of a dataset to do any real training on. As a way of combatting this, I broke each time series into a set of sliding windows (see Figure \ref{fig:sliding-window}). A window represents a sequence of a chosen length; in this case I chose 24 as the window length, as a 24 year period is a long enough time to observe macroeconomic changes while still allowing for the data to be divided up into many sequences. My stride length for the sliding windows was 1, so the start time of each window was incremented by 1. After cleaning to ignore windows which were missing datapoints, this left me with 155 windows available for training, with some withheld for testing. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{sliding_windows.png}
    \caption{Visualization of sliding window approach to data batching. Colored boxes represent windows of length 4, with stride length 1. Adapted from \textcite{zhan2024versatile}.}
    \label{fig:sliding-window}
\end{figure}

Though windows will not all be independent of each other, this method allows the network to learn the dynamics of short sequences through exposure to a much higher number of sequences. With a [9, 56, 4] training dataset the network is exposed to 9x56=504 training steps. On a [64, 24, 4] training batch size as I used for many of my tests, the model is exposed to 64x24=1536 training steps. Even though there will be overlap, using 64/155 randomly sampled batches at a time minimizes overlap, and the increased training steps is worth the slight increase in risk of overfitting.

\subsection{TimeGAN Implementation}
For the actual TimeGAN model implementation itself I mostly followed the original repository, with a few unique implementation decisions to hopefully benefit my small dataset use case. The model is built on the TensorFlow library, with the data represented as tensors and the neural networks being built into as a tensor graph. Due to updates in the TensorFlow library since the original TimeGAN paper, some structural changes were needed to switch to Keras versioning from the original. These changes affect the way some calls are made but do not affect the performance/implementation of the model.

For clarity's sake, I decided to move the actual neural cell instantiation into a function in utils.py rather than call it directly from tensorflow in neural networks as done in the original repository. This allowed me to experiment with changing RNN cell type within utils.py without having to change it individually in each network. 

For connecting the neural network I decided to use a Xaviar Initialization on the weight matrix for the neural connections. This is done to introduce some slight variation and randomness into the weight matrix for each training iteration of the neural networks. A Xaviar Init takes input of a shape, and outputs a random weight matrix with slight noise added for connecting the nodes within that shape. Since the scale and variation of the macroeconomic indicators used is not consistent, and training was done on a space normalized from [-1,1], the noise ensures that no one feature is being overemphasized across the entirety of training in this compressed space.

For the same reasons, I use an Adam Optimizer called on the autoencoder, generator, and discriminator to optimize training for minimizing loss functions. The Adam Optimizer is an adaptive learning rate optimization algorithm (standard TensorFlow function) that takes an input of a learning rate and will then adjust the learning rate between features during training by that input rate if it detects that certain features are being over or under emphasized. 

\subsection{Study Method}
In conducting my study after the base implementation I will conduct evaluation on the synthetic generated data using the evaluation metrics \ref{sec:evaluation-metrics} . I will then observe the primary areas where I can improve on the performance of my model, and make an informed decision on parameter updates for the next training iteration. I will repeat the process until I have experimented in a meaningful way with all training parameters, and have a set of informed changes that improve TimeGAN performance on a small dataset. I will not attempt to minutely adjust parameters to reach an optimal set, but will focus on what adjustments improve performance specifically to my small dataset use case. 

If I make a parameter change and see no noticeable improvement in the evaluation metrics for the synthetic data produced by that training cycle then I will discard the change. However, if there are not improvements in the synthetic data but there are notable improvements in loss values throughout training then I will keep the adaptation as an improvement in training health. Any parameter adjustments which have a notably positive impact I will treat as part of the new baseline, and will carry forward into the next trial.

After implementing the model and performing my iterative experiments over parameter adjustments, I hope to have a set of justified, meaningful changes that correlate to improved synthetic data quality. Through this approach I hope to justify that it is possible to optimize TimeGAN over small datasets to effectively generate synthetic data, as well as identify areas for future improvements in TimeGAN performance.

\section{Evaluation Metrics}
\label{sec:evaluation-metrics}

My goal through evaluation is to statistically quantify the ability of the synthetic data to maintain distributional and temporal features while not simply copying or overfitting training data. To achieve this I selected a number of evaluation metrics, as well as implemented a train-synthetic test-real approach as guided by prior work.

\subsection{Feature Distribution: Kolmogorov-Smirnov}
In order to evaluate the per-feature distribution of my synthetic data the first test I will use is the Kolmogorov-Smirnov (KS) test. The KS test is a cumulative frequency distribution (CFD) comparison test, used by \textcite{bourou2021reviewtabulardata} to evaluate the performance of GANs in generating synthetic data. Their approach inspired me to use the KS test. The KS test finds the maximum vertical distance between two CFDs – in this case a real CFD and synthetic CFD (see Figure INSERT FIGURE NUMBER). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{KS2_Example.png}
    \caption{Visualization of Kolmogorov-Smirnov test between cumulative frequency distributions. From \textcite{wikipedia_kstest}.}
    \label{fig:ks-test}
\end{figure}

KS then uses this maximum distance calculation to perform a hypothesis test that the two distributions are not significantly different. If the p-value calculated by the test is less than your desired significance level $\alpha$, then you can reject the null hypothesis that the distributions are not significantly different, and conclude that the sample distribution does not come from the original distribution. The KS test was run on each feature in the dataset, to give a sense of distributional similarity on a per-feature level.

\subsection{Feature Distribution: Wasserstein Distance}
The second feature distribution test that I will use is Wasserstein Distance, inspired by \textcite{livieris2024anevaluationframework} approach to synthetic data evaluation. The WD is a measurement of the absolute difference or cost to transcribe one distribution onto another. Unlike the KS test which only looks at the individual point of highest difference, the WD looks at the total difference between CFDs. Like the KS test, I use a WD test on a per-feature basis to help evaluate whether one distribution came from another, in my case whether the synthetic distribution was derived from the real distribution. 

You can use WD for hypothesis testing purposes, or since it is revealing of the difference between CFD as a whole, you can also interpret it directly in absolute terms. Between WD and KS tests, I will measure both the maximum and total cumulative difference between real-synth distributions. Since I know that the synthetic distributions came from the real, I will not be using these measurements for hypothesis testing reasons, but simply as a revealing metric about the difference between distributions.

\subsection{Cross Feature Correlation}
From \textcite{marti2020corrgan}, who applies adversarial techniques to sampling on financial correlation matrices, I took inspiration to use the Frobenius norm of correlation matrices to calculate cross feature correlation. A correlation matrix is a DxD matrix of features with each feature’s correlation coefficient being stored in the corresponding box. In my case the correlation matrices would look like the following:

\begin{table}[ht]
    \centering
    \caption{Correlation Matrix of Macroeconomic Indicators}
    \label{tab:correlation-matrix}
    {\small%
    \setlength{\tabcolsep}{4pt}%
    \renewcommand{\arraystretch}{0.95}%
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c c}
        \hline
         & \textbf{GDP Growth} & \textbf{Inflation (I)} & \textbf{...} \\
        \hline
        \textbf{GDP Growth}     & 1                          & $\mathrm{corr(GDP,I)}$   & ... \\
        \textbf{Inflation (I)} & $\mathrm{corr(I,GDP)}$ & 1                          & ... \\
        \textbf{...}            & ...                        & ...                        & ... \\
        \hline
    \end{tabular*}%
    }%
\end{table}


With there being one correlation matrix for both real vs real and one for synthetic vs synthetic distributions. These correlation matrices are then compared using the Frobenius norm, which calculates the same total difference between matrices in the same conceptual manner as calculating Euclidean distance between vectors. Though there is not a standard to aim for in terms of Frobenius score, lower is better as it implies that the relationships between features are more similar. I will be comparing Frobenius norms across tests as one of my essential tests. A lower Frob. norm implies that the datasets have more realistic inter-feature relationships.

\section{Results and Discussion}
\subsection{Baseline Model Evaluation}
My first test of the complete model used only the set of default parameters listed in Table \ref{table:default-params} (Appendix \ref{appendix:architecture-overview}). As a baseline I ran the evaluation metrics over comparisons between real and synthetic data, and between real training data and withheld real data. This established a real-vs-real benchmark for the evaluation methods: realistic values across all metrics indicated the procedures were sound, and parameter adjustments that moved model performance closer to this benchmark were treated as improvements.

\begin{table}[t]
    \centering
    \caption{Comparison of evaluation metrics between the real--real baseline and the initial synthetic baseline (v0).}
    \label{table:baseline-v0-results}
    {\small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{0.95}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c}
        \toprule
        \textbf{Metric} & \textbf{Real Baseline} & \textbf{v0 Synthetic} \\
        \midrule
        KS Max              & 0.875  & 0.975  \\
        WD Mean             & 0.174  & 0.206  \\
        WD Max              & 0.341  & 0.384  \\
        Frob Diff           & 1.438  & 3.432  \\
        ACF RMSE            & 0.174  & 0.155  \\
        TSTR MSE Mean       & 0.037  & 1.096  \\
        KNN Mean S--R       & 14.361 & 12.728 \\
        KNN Mean R--S       & 13.725 & 15.755 \\
        KNN Asymmetry       & 0.318  & -3.144 \\
        \bottomrule
    \end{tabular*}
    }
\end{table}


The only evaluation metric which seemed erroneous was the Kolmogorov--Smirnov (KS) test: even the real--real comparison in Table \ref{table:baseline-v0-results} reports a KS Max of 0.875, which corresponds to an effectively zero p-value. I attribute this to large regime shifts and differing starting conditions across countries and periods—sliding windows that include shocks or structural breaks produce markedly different empirical CDFs, yielding high KS distances even between distinct real samples. Therefore KS is of limited use here as a strict hypothesis test and is treated instead as a descriptive distance measure.

Overall my synthetic baseline (v0) test performs similarly to the real-to-real comparison on several marginal and temporal metrics, but performs significantly worse on realism and predictive scores. Though both KS Max scores are high enough that you can reject $\alpha$ at any standard significance level, the v0 score was significantly higher, indicating poor distributional matching. This could be due to differences in starting conditions, as KS test is sensitive to temporal differences. Both WD measurements are higher in v0, but are relatively similar (Table \ref{table:baseline-v0-results}). This indicates that the model is getting rough distributional shapes, but is performing poorly at more extreme values (higher WD Max). Thus marginal distributions are not well matched, but distributional closeness is not drastically different from the real baseline.

There was a large disparity in performance in the Frobenius difference in correlation matrices. The real baseline had a Frob score of 1.438, while v0 had a significantly higher score of 3.432 (Table \ref{table:baseline-v0-results}). This indicates that the v0 synthetic data does not at all mimic the covariance structure (inter-feature relationships) of the real data well. Surprisingly v0 has slightly better ACF RMSE than the real-real baseline. Most likely this is due to the fact that early GAN outputs could be oversmoothed, generating series without much temporal variation that creates artificially low autocorrelation, but is also part of the reason for the poor performance in Frobenius difference. 

The most important difference between the two tests for my sake is the poor TSTR performance. The TSTR MSE of the v0 test was 1.096, significantly worse than the real baseline of 0.037 (Table \ref{table:baseline-v0-results}). The poor predictive score indicates that the synthetic data cannot reasonably be used to predict real data relationships. The synthetic data is not realistic enough for any sort of downstream application, which is how \textcite{yuan2024multifacetedevaluation} defines the real-world utility of synthetic data.

In terms of novelty the scores are somewhat hard to interpret because it is a measurement of average absolute Euclidean distance over 4 dimensional datapoints. It seems however that a score of about 14 is realistic given the results of the real-real comparison. What stands out is that for the v0 test, there was a much higher degree of asymmetry (-3.144 vs 0.318) between the real-fake and fake-real KNN means (Table \ref{table:baseline-v0-results}). This implies that the synthetic data is asymmetric, meaning that v0 synthetic samples cluster in certain ranges (lower synthetic to real score) but fail to cover the full range of real data (higher real to synthetic score).

\subsection{Final Results: Tuned Model}

Overall from my initial baseline v0 to my final trial v14 there was substantial improvement across distributional, temporal, discriminative, and most importantly downstream application performance. The final positive incremental change of increasing hidden dimensions from 24-32 helped stabilize and enhance performance, balancing the distributional improvements of v4 with the predictive success of v10, while also having the lowest novelty asymmetry of all trials (See TABLE A, TABLE B).

\begin{table}[t]
    \centering
    \caption{Performance improvements from the initial model (v0) to the tuned model (v14).}
    \label{table:v0-v14-improvement}
    {\small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{0.95}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l c c c}
        \toprule
        \textbf{Metric} & \textbf{v0} & \textbf{v14} & \textbf{Improvement} \\
        \midrule
        KS Max            & 0.975  & 0.975  & 0\%      \\
        WD Mean           & 0.206  & 0.183  & 12.56\%  \\
        WD Max            & 0.384  & 0.362  & 6.07\%   \\
        Frob Diff         & 3.432  & 2.621  & 30.94\%  \\
        ACF RMSE          & 0.155  & 0.287  & --45.99\% \\
        TSTR Mean         & 1.096  & 0.588  & 86.39\%  \\
        KNN Mean S--R     & 12.728 & 13.202 & --       \\
        KNN Mean R--S     & 15.755 & 15.611 & --       \\
        KNN Asymmetry     & -3.144 & -2.409 & 30.51\% \\
        \bottomrule
    \end{tabular*}
    }
\end{table}

The hidden dimensions directly control the capacity of the embedder, supervisor, generator, and recovery networks exposure to previous hidden states. A higher hidden dimension improved Frobenius difference, helping latent transitions become more realistic. It also better allows the supervisor to model lagged relationships, which helps inform how the generator generates realistic synthetic time steps. 

Compared to v0, WD mean and max showed a moderate improvement, of 12.56\% and 6.07\% respectively–indicating that the model matches the shape of the real distributions better, and that distributions are not as narrowly clustered as in v0 (Table \ref{table:v0-v14-improvement}). There was a large improvement (30.94\% better performance) in Frobenius difference, indicating that cross-feature correlation was significantly closer to the real-real baseline (Table \ref{table:v0-v14-improvement}). This implies that the synthetic data has much more realistic intra-feature relationships than the initial v0 version. 

The only metric where v14 actually performed worse than v0 was the autocorrelation structure (ACF RMSE). The change from an ACF RMSE of 0.155 in v0 to 0.287 in v14 indicates that there was more temporal volatility in v14 (Table \ref{table:v0-v14-improvement}). While this is a significantly worse score, in terms of my goals it is not necessarily concerning. The initial ACF RMSE score for v0 was very low, lower even than the real-real baseline (Table \ref{table:v0-v14-improvement}). This implies that the initial ACF RMSE measurements may have over-smoothed the temporal relationships, when in reality they may be a little more erratic. An ACF RMSE score of 0.287 is not concerningly high given that there were marked improvements in distribution, cross feature correlation, predictive scores, and novelty.

The most significant improvement from v0 to v14 is in novelty, where TSTR Mean (across features) changed from 1.096 in v0 to 0.588 in v14 (Table \ref{table:v0-v14-improvement}). This marks an 86.39\% better performance in predictive ability, which I consider the most important metric for my case. The v14 trial was 86.39\% more accurate than v0 when it came to using synthetic data to predict the next time step in the real data, which indicates that with further refinement it may be possible to reach a point where synthetic data serves as a valid proxy for real data for downstream machine learning applications.

Additionally the v14 test had significantly the lowest KNN asymmetry of all tests. The value of KNN means was not drastically different than in other tests, but the S-R mean and R-S mean only had an asymmetry of -2.409 (Table \ref{table:v0-v14-improvement}). As asymmetry moves closer to zero that means that the synthetic data more uniformly covers the real data distribution and has less of a tendency to cluster together. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{v14results.png}
    \caption{GDP Growth vs Unemployment Rate for v14 Synthetic vs Real Data}
    \label{fig:v14-results}
\end{figure}

Overall between v0 and v14 the improvements almost across the board in evaluation metrics demonstrate that it was possible to take a stock implementation of the TimeGAN model and refine it for my use case. Visually the v14 distribution is clearly still clustered around the mean, which makes sense as it is easy for a model with exposure to a small amount of data to minimize average loss by just generating average values (see \ref{fig:v14-results}). However, the various iterations of parameter adjustments show that novelty and realism does not have to be a trade off, and with the right adjustments the model can be tuned to perform better on small datasets. I do not think that the v14 model is downstream application ready. In fact I would say that until a TSTR score ~0.037 (see \ref{table:baseline-v0-results}) is reached, then the synthetic data cannot be considered to have a real empirical application. As models continue to improve however, I do believe that the notable improvement in performance indicates that adversarial techniques could viably produce synthetic data from small training sets. The ability to optimize across distributional, temporal, and predictive performance through the combined autoencoder, supervised, and adversarial objectives demonstrates the potential future application of the TimeGAN model for accurately capturing the subtle temporal relationships between macroeconomic indicators.

\subsection{Avenue for Future Study}
For future research I would like to compare the performance of the TimeGAN model on small datasets to other non-adversarial methods. Throughout this process I observed that adversarial learning may too heavily penalize generations that are different from the mean when trained on such small datasets. Generating synthetic data at the mean of a distribution is the easiest way to produce stable, low average loss. I noticed that the efforts that I made to decrease the influence of the adversarial component of the model, such as adding noise to the discriminator and increasing gamma lead to some of the most significant improvements in model performance.

I am curious if perhaps a framework like variational autoencoders (VAEs) or sequential VAEs could be applied to synthetic time series generation. Training may be more stable without a discriminator, which from my process I observed to be perhaps too powerful of a network for small datasets. VAEs are probabilistic models which learn a latent probability distribution and could then use that to generate then decode a synthetic latent distribution. In this approach it is somewhat the opposite of TimeGAN, which takes in a random distribution and attempts to generate it into a realistic synthetic distribution.  

In retrospect, I also think that my research would be more applicable to generating data from taking an input of initial conditions. When I think of macroeconomic data and decision making, it is all based around what are the expected future movements of macroeconomic indicators. In my case, random macroeconomic datasets are not particularly useful. They may have better downstream applications than a specific prediction based on initial conditions, but overall are less likely to be applicable to actual decision making. It also would take some of the pattern recognition responsibility out of the model, as it would be given a set of realistic initial points to then generate from. My biggest concern with such a model is that you would need substantial training data to learn patterns from different initial conditions.


\section{Ethical Considerations}
As the overall goal of my project was to generate synthetic macroeconomic data capable of being used as a proxy for real data, it is important to assess the ethical implications of such a project. In a hypothetical scenario where synthetic macroeconomic data was to become widely used by economists/bankers, the primary ethical concern with my project becomes the question of who actually benefits from improved economic simulation, and who may be unintentionally excluded from those benefits.

\textcite{ravallion2021macroeconomicmisery} point out how changes in macroeconomic conditions have disproportionately negative impacts on the poorest Americans. Unemployment rate or inflation rates for example, may negatively impact the value of the wealth of upper class Americans, but won’t have any impacts on their daily lives \textcite{ravallion2021macroeconomicmisery}. The poor however, are placed under immense burden from macroeconomic changes.\\

\indent{}The major concern is that improved macroeconomic modelling only serves to benefit those with the financial literacy and wealth to take advantage of it–particularly banks, policymakers, and wealth investors. As such, macroeconomic modelling is likely to be used to benefit these upper-class groups, rather than improve the financial standing and financial literacy of the lower class.

\indent{}There is also the risk that synthetic data is used by a powerful actor (e.g. the Federal Reserve System in deciding interest rates) without disclosure of where that data came from or why it was used. Since macroeconomic factors are impactful to all, anybody who is in a policy making position must be transparent and accountable for the data they use. The general public, and even data-scientists, may not know exactly what patterns an algorithm is learning in training. There is the potential that synthetic data abstracts real world conditions, creating incomplete or untrustworthy datasets that are then used in vital decision making. \\

\indent{}On the topic of ethics in computer science, the use of algorithmic decision making in credit-scoring and loan rating is controversial, as it obscures the process behind making very impactful decisions on people's lives. The use of synthetic macroeconomic data for decision making poses the same risks–obscuring the factors in decisions which impact people's lives. Additionally, if any bias is present in the dataset that is used to produce the synthetic data, then the bias patterns will be present in the synthetic data as well. In loan rating for example, it is possible that if you come from a demographic which more frequently has their loan applications rejected, then that bias will carry into the algorithm. If a synthetic data production model was trained on the same input data, then the synthetic results would also carry that bias, and overly reject loan applicants from said demographic. When it comes to real people's lives, using synthetic data becomes a real liability.

\section{Conclusion and Future Work}
Throughout this project I was able to make notable improvements in the performance of the TimeGAN model on generating synthetic data from a small input dataset. Both the statistical and TSTR scores improved from my v0 to v14 tests. However, I was unable to fully prevent overfitting to the input data and the tendency to collapse toward the mean. This is due, as I discovered, to the inherent conflict between generating novel, varied distributions, and matching the statistical properties of the original dataset. I was able to refine this tradeoff to an extent, and produce synthetic data in v14 which demonstrated both better novelty and distributional scores than in v0. Ultimately though I would conclude that in its current state, the size of the training dataset is too severe a limiting factor in the ability to produce temporally realistic and distributionally viable synthetic datasets using TimeGAN.

If a model were to reach the goal of producing realistic synthetic macroeconomic data to the extent that said synthetic data was indistinguishable from real, then this raises a number of ethical concerns. Aside from the obvious like fabricating data, if applied to macroeconomic modelling techniques there is a risk of further exacerbating the divide between those who have the knowledge and wealth to take advantage of financial information and those who do not. Additionally, if synthetic data is used to help inform or train decision making in financial experts, there is the risk that they make decisions that impact real people's lives based on a made up scenario. Transparency on the generation process of synthetic data is vital to mitigating these risks.


% Defer appendix material until after the bibliography so the bibliography
% appears as part of the main paper and the following sections become Appendix.
\AtEndDocument{%
    \clearpage
    \appendix
    \section{Replication Instructions}
    \section{Architecture Overview}
    \label{appendix:architecture-overview}

    \subsection*{Default Training Parameters}

    \begin{table}[h]
        \centering
        \caption{Default training parameters used in the baseline TimeGAN implementation.}
        \label{table:default-params}
        \begin{tabular}{ll}
        \toprule
        \textbf{Model Parameter} & \textbf{Default Value} \\
        \midrule
        $L$ (sequence length)     & 24 \\
        $D$ (feature dimension)   & 4 \\
        $z_{\text{dim}}$          & 4 \\
        batch\_size               & 64 \\
        ae\_warmup\_it            & 600 \\
        gan\_iters                & 3000 \\
        $\gamma$                  & 1.0 \\
        learning\_rate            & 0.001 \\
        module                    & \texttt{"gru"} \\
        hidden\_dim               & 24 \\
        num\_layers               & 2 \\
        \bottomrule
        \end{tabular}
    \end{table}

    These are the training parameters passed into the model during training. They are also the primary hyperparameters varied during experimentation.

    \begin{itemize}
        \item \textbf{$L$}: Length of each sliding window (number of time steps). In this project, $L = 24$ corresponds to a 24-year sequence.
        
        \item \textbf{$D$}: Number of dimensions in the training data (number of economic indicators). This was fixed at $D = 4$ for GDP growth, inflation rate, unemployment rate, and population growth.
        
        \item \textbf{$z_dim$}: Dimensionality of the random noise vectors fed into the generator, controlling generative diversity.
        
        \item \textbf{batch\_size}: Number of sliding windows passed to the model during each training step.
        
        \item \textit{ae\_warmup\_it}: Number of autoencoder warmup iterations prior to adversarial training.
        
        \item \textit{gan\_iters}: Number of full GAN training iterations.
        
        \item \textbf{$\gamma$}: Weighting coefficient balancing the supervised loss and adversarial loss components within the generator loss.
        
        \item \textbf{learning\_rate}: The step size used during optimization; controls how quickly model weights update during backpropagation.
        
        \item \textit{module}: The type of recurrent cell used in the model’s networks. Supported options include:
        \begin{itemize}
            \item \texttt{``gru''}: Gated Recurrent Unit; uses an update gate (memory retention) and reset gate (context forgetting). More lightweight and computationally efficient.
            \item \texttt{``lstm''}: Long Short-Term Memory; uses three gates and maintains a cell state, offering greater robustness at higher computational cost.
        \end{itemize}
        
        \item \textit{hidden\_dim}: Size of the hidden state in each neural network layer; determines the number of latent features used to encode temporal structure.
        
        \item \textit{num\_layers}: Number of stacked layers in the recurrent neural network architecture.
    \end{itemize}
}

\nocite{*}
\printbibliography

\clearpage

\onecolumn

\end{document}