\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

\bibliography{references.bib}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series)
    /Author (Kincaid Fries)
}

% set the title and author information
\title{TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series}
\author{Kincaid Fries}
\affiliation{Occidental College}
\email{kfries@oxy.edu}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Background}
The use of machine learning in financial prediction has grown rapidly due to its ability to model complex, multivariable relationships and generate realistic predictions. Machine learning is particularly effective in equities markets where thousands of points of training data are available daily for stock movements. In contrast, macroeconomic data takes months of study, surveying, and research to come up with even a single realistic measurement. Consequently, macroeconomic forecasting models rely on quarterly historical data at best. \textcite{baltazar2020sustainableeconomies} note that small datasets affect the reliability of macroeconomic modelling. Other challenges like changes in calculation methods, unavailability of data for certain time periods, and changes in indicator definitions make it hard to train macroeconomic models without overfitting to small datasets. Recent advances in synthetic data generation offer a promising alternative, allowing researchers to create realistic simulations of economic behavior without relying solely on historical data.

\subsection{Goal}
The goal of my senior comprehensive project is to evaluate whether a machine learning model trained on small macroeconomic datasets can be used to generate realistic synthetic time series. By normalizing macroeconomic indicators across different economies, I hope to address the issue of relatively limited time-series for a single country by taking training data from multiple countries. My project will explore whether synthetic time-series produced by a model trained on data from multiple countries could potentially address the problem of lack of data availability, and serve as a valid counterpart to real economic data in forecasting applications.

Abstracting from this project, data availability is becoming an increasingly prevalent issue as the technological world embraces machine learning. Major data companies like Meta and OpenAI have faced lawsuits over data piracy, and smaller companies want to utilize the power of machine learning to drive decision making, but may face issues with security or compliance that prevent them from training models. If we had the power to take a small dataset and extend it, or create realistic alternative datasets using synthetic data generation, then a number of these problems could be solved. Security, privacy, and copyright could be better preserved while still having access to the quantity of data needed to train machine learning models. 

\subsection{Approach}
I will implement a TimeGAN model, which combines a supervised autoencoder to learn temporal dynamics with a generative adversarial network (GAN) to generate realistic synthetic data \textcite{yoon2019timeseriesgenerative}. The model will be trained using macroeconomic data taken from developed, free-market economies, to ensure there is consistency in the underlying macroeconomic trends it identifies. To evaluate whether the generated synthetic data is realistic, I will compare the properties of the synthetic data with real world data using a number of statistical tests focused on distributional similarity, cross time similarity, and novelty. Additionally, I will examine the performance of a forecasting model using the train-synthetic test-real (TSTR) method–training a simple forecasting model on the synthetic data to predict the next step in the real data. Through TSTR evaluation I will be able to assess the potential viability of using synthetic data in training models for real data tasks.

By looking at both statistical factors and forecasting model performance, I hope to have a comprehensive framework for assessing the viability of the synthetic data. I will then iterate over the model, adjusting training parameters in order to see what areas are particularly effective for model optimization on small datasets.

\section{Technical Background}

\section{Prior Work}

First and foremost, my project relies heavily on the original TimeGAN implementation by \textcite{yoon2019timeseriesgenerative}. They identify that on their own, GANs “do not adequately attend to the temporal correlations unique to time-series data”, while purely supervised models are “inherently deterministic” (\textcite{yoon2019timeseriesgenerative}, 1). Their proposal for addressing this disparity was to create an interwoven framework that provides both the benefits of supervised models and the adversarial component of GANs. However, \textcite{yoon2019timeseriesgenerative} are not the only researchers who aim to optimize the performance of a GAN across temporal features. The unique aspect of their project that stood out to me was the fact that the model is trained on batches of data which is ideal for training the model on macroeconomic conditions using multiple different countries for input. 

Aside from the original TimeGAN repository, my project draws inspiration from studies that reflect the technical, theoretical, and evaluative aspects of synthetic data generation and machine learning applications in economics. The first applies GANs to generating synthetic financial scenarios (\textcite{rizzato2023generativeadversarial}). Another applies a macroeconomic model to loan default rate predictions (\textcite{baltazar2020sustainableeconomies}). Finally, \textcite{yuan2024multifacetedevaluation} and \textcite{livieris2024anevaluationframework} provide a reference for how to go about my evaluation metrics and results discussion.

\subsection{Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation}
\textcite{rizzato2023generativeadversarial} propose a new generative adversarial network approach to synthetic financial scenario generation, which they call Jinkou. Jinkou was made to generate realistic synthetic time series datasets on the movement of equities under different macroeconomic conditions \cite{rizzato2023generativeadversarial}. Unlike the original TimeGAN paper, they introduce a macroeconomic element which offers a similar conceptual approach and methodology to my own goals for this project. 

\subsubsection{Technical Approach}
They first took real datasets on equities with specific variables related to stock performance, and augmented that dataset with global state variables describing macroeconomic conditions \cite{rizzato2023generativeadversarial}. Their proprietary Jinkou method uses a combination of a bidirectional GAN (BiGAN) and conditional GAN (cGAN) to improve the probability distribution of the synthetic data \cite{rizzato2023generativeadversarial}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{bigan_vs_cgan.png}
    \caption{Visual comparison of BiGAN and Conditional GAN training architectures. Adapted from \textcite{rizzato2023generativeadversarial}.}
    \label{fig:bigan-vs-cgan}
\end{figure}

The BiGAN maps latent space to the data space with an autoencoder, similar to the embedder and recovery networks of the TimeGAN. The synthetic time series generated by the trained BiGAN are used as the input for the cGAN, which learns to map each datapoint \textit{y} to a probability score \textit{p} drawn from the dataset distribution of \textit{P} \cite{rizzato2023generativeadversarial}. The cGAN refines the results of the BiGAN by more accurately mapping the data to the probability distribution expected of the data. They perform statistical analysis of their synthetic data generated under four different market conditions (bull market, bear market, volatile market, and debt crisis). They evaluated the synthetic change in stock price from start to end of the time series with the real percent change in stock price \cite{rizzato2023generativeadversarial}. They found similar values for synthetic vs real data, typically with a difference between [-1,2]\%, indicating that their synthetic data was sufficiently accurate to the real data.

\subsubsection{Key Takeaway}
This project demonstrates how synthetic data is a viable way of simulating market scenarios under different macroeconomic conditions. Specific to my project, \textcite{rizzato2023generativeadversarial} found the use of GAN models to be effective for generating synthetic time-series, and were able to compare that synthetic data to real data with a satisfactory degree of accuracy. Their results validate the importance of macroeconomic indicators in financial modelling.

\subsection{Sustainable Economies: Using a Macroeconomic Model to Predict How the Default Rate is Affected Under Economic Stress Scenarios}
\textcite{baltazar2020sustainableeconomies} build a macroeconomic stress-testing framework to predict loan default rates under hypothetical, simulated macroeconomic conditions–a framework they argue would allow banks and policy makers to improve their decision making in economic downturns \textcite{baltazar2020sustainableeconomies}. They use a predefined macroeconomic model rather than machine learning, having that model generate random Monte Carlo simulations \textcite{baltazar2020sustainableeconomies}.\\

Since this is not a machine learning project, my takeaways were not technical but theoretical. They validate the importance of GDP and CPI as revealing macroeconomic indicators, and acknowledge the problem of small datasets for macroeconomic forecasting \textcite{baltazar2020sustainableeconomies}. This study confirmed the relevance and importance of everything I am working on for my project: the use of macroeconomic indicators for understanding economic conditions, the downstream effects of macroeconomic conditions on all aspects of finance and the economy, and the necessity and potential applications of synthetic macroeconomic data in augmenting existing datasets.

\subsection{A Multi-Faceted Evaluation Framework for Assessing Synthetic Data}
This study, by \textcite{yuan2024multifacetedevaluation}, proposes a novel method for the evaluation of synthetic datasets, which they call SynEval. Their evaluation method has three dimensions–fidelity, utility and privacy–of which I am interested in ‘utility’, their measure of the applicability of the synthetic data \cite{yuan2024multifacetedevaluation}.

\subsubsection{Technical Approach}
\indent{}\textcite{yuan2024multifacetedevaluation} refer to the ‘utility’ of a synthetic dataset as its ability to be used downstream for machine learning. This aspect is what I am most concerned with, as my goal is to create synthetic data that is academically and empirically useful. Downstream machine learning refers to the application of the generated data to training/testing machine learning models. The utility evaluation they proposed is based on a framework referred to as TSTR (Train-Synthetic-Test-Real) \cite{yuan2024multifacetedevaluation}. They perform a study on synthetic data generated by popular LLMs ChatGPT 3.5, Claude 3 Opus, and Llama 2 13B. In this case, they used 50 samples from a software product review dataset to train LLM’s to generate synthetic review-rating data \cite{yuan2024multifacetedevaluation}. They then trained four sentiment classification models; one on the real dataset, and the other three on the synthetic datasets produced by the LLMs \cite{yuan2024multifacetedevaluation}. The sentiment classification models were evaluated on unseen test data, with Mean Absolute Error (MAE) and percentage accuracy of correct classifications used to evaluate the performance of each model. 

\subsubsection{Key Takeaway}
In evaluating the synthetic datasets on downstream machine learning performance, they found that the model trained on synthetic data generated by Claude had a MAE of 1.2929 and accuracy of 67.68\%, which was very comparable to the model trained on real data (MAE 1.3019, accuracy 67.92\%) \cite{yuan2024multifacetedevaluation}.\textcite{yuan2024multifacetedevaluation} results prove that synthetic data can be effectively used on downstream machine learning tasks. Their methodology of using downstream machine learning as an evaluation metric for the quality of synthetic data validates my plan of using a forecasting model trained on synthetic versus real data as an evaluation metric. 

\subsection{An Evaluation Framework for Synthetic Data Generation Models}
In the research performed by \textcite{livieris2024anevaluationframework}, they compared a number of different statistical methods specifically for evaluating how realistic synthetic data is to real data. They compare data generated by five synthetic data generation models using standard tests. The three tests they used which I find most relevant to my project were Wasserstein-Cramer's V-test, novelty test, and anomaly detection test \cite{livieris2024anevaluationframework}. 

\begin{itemize}
    \item \textit{Wasserstein-Cramer's V Test}: a test that combines the Wasserstein distance–the quantification of the distance between the probability distributions of the real and synthetic data–with Cramer’s V to account for the distributions across different variables \cite{livieris2024anevaluationframework}. The test evaluates if synthetic data matches the distribution of real data accounting for shape and variance, with low scores being better. Since Wasserstein-Cramer's V Test scores are not bounded to a range, it is somewhat arbitrary determining what is a good score or not. 
    \item \textit{Novelty Test}: a measure of the synthetic data model’s ability to generate new situations not present in the original dataset. It is calculated as the ratio of unmatched instances between the synthetic dataset, where an instance is considered match if $\left| s_i - r_i \right| < \alpha$,
    with $s_i$ representing a synthetic datapoint, $r_i$ representing a real point, and $\alpha$ being some predefined threshold \cite{livieris2024anevaluationframework}. 
    \item \textit{Anomly Detection Test}: an isolation forest is trained on the real data, then applied to the synthetic dataset, to which it assigns an abnormality score to each point \cite{livieris2024anevaluationframework}. A high degree of abnormality would mean that the synthetic dataset has a number of points that do not match the distribution of the real data. An isolation forest is an unsupervised model which isolates anomalies from a dataset using a series of decision trees.
\end{itemize}
\subsubsection{Key Takeaway}
While the anomaly detection test is interesting, I think within my overall framework of evaluating distributional similarity, it is not necessary, as anomalous synthetic data will be clearly visible from other evaluation metrics. The Wasserstein Distance however, I will use as part of my distributional evaluation. Implementing the combination with Cramer’s V-test is not a commonly accepted practice and makes the results a bit less interpretable, so I will focus just on Wasserstein Distance rather than the combined test. Additionally, I will implement the novelty test through a simple nearest neighbors Euclidian distance calculation.


\section{Methods}

\section{Evaluation Metrics}

\section{Results and Discussion}

\section{Ethical Considerations}
As the overall goal of my project was to generate synthetic macroeconomic data capable of being used as a proxy for real data, it is important to assess the ethical implications of such a project. In a hypothetical scenario where synthetic macroeconomic data was to become widely used by economists/bankers, the primary ethical concern with my project becomes the question of who actually benefits from improved economic simulation, and who may be unintentionally excluded from those benefits.

\textcite{ravallion2021macroeconomicmisery} point out how changes in macroeconomic conditions have disproportionately negative impacts on the poorest Americans. Unemployment rate or inflation rates for example, may negatively impact the value of the wealth of upper class Americans, but won’t have any impacts on their daily lives \textcite{ravallion2021macroeconomicmisery}. The poor however, are placed under immense burden from macroeconomic changes.\\

\indent{}The major concern is that improved macroeconomic modelling only serves to benefit those with the financial literacy and wealth to take advantage of it–particularly banks, policymakers, and wealth investors. As such, macroeconomic modelling is likely to be used to benefit these upper-class groups, rather than improve the financial standing and financial literacy of the lower class.

\indent{}There is also the risk that synthetic data is used by a powerful actor (e.g. the Federal Reserve System in deciding interest rates) without disclosure of where that data came from or why it was used. Since macroeconomic factors are impactful to all, anybody who is in a policy making position must be transparent and accountable for the data they use. The general public, and even data-scientists, may not know exactly what patterns an algorithm is learning in training. There is the potential that synthetic data abstracts real world conditions, creating incomplete or untrustworthy datasets that are then used in vital decision making. \\

\indent{}On the topic of ethics in computer science, the use of algorithmic decision making in credit-scoring and loan rating is controversial, as it obscures the process behind making very impactful decisions on people's lives. The use of synthetic macroeconomic data for decision making poses the same risks–obscuring the factors in decisions which impact people's lives. Additionally, if any bias is present in the dataset that is used to produce the synthetic data, then the bias patterns will be present in the synthetic data as well. In loan rating for example, it is possible that if you come from a demographic which more frequently has their loan applications rejected, then that bias will carry into the algorithm. If a synthetic data production model was trained on the same input data, then the synthetic results would also carry that bias, and overly reject loan applicants from said demographic. When it comes to real people's lives, using synthetic data becomes a real liability.

\section{Conclusion and Future Work}

% Defer appendix material until after the bibliography so the bibliography
% appears as part of the main paper and the following sections become Appendix.
\AtEndDocument{%
    \clearpage
    \appendix
    \section{Replication Instructions}
    \section{Architecture Overview}
}

\printbibliography

\clearpage

\onecolumn

\end{document}