\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

\bibliography{references.bib}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series)
    /Author (Kincaid Fries)
}

% set the title and author information
\title{TimeGAN Optimization on Small Datasets: Generating and Evaluating Synthetic Macroeconomic Time Series}
\author{Kincaid Fries}
\affiliation{Occidental College}
\email{kfries@oxy.edu}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Background}
The use of machine learning in financial prediction has grown rapidly due to its ability to model complex, multivariable relationships and generate realistic predictions. Machine learning is particularly effective in equities markets where thousands of points of training data are available daily for stock movements. In contrast, macroeconomic data takes months of study, surveying, and research to come up with even a single realistic measurement. Consequently, macroeconomic forecasting models rely on quarterly historical data at best. \textcite{baltazar2020sustainableeconomies} note that small datasets affect the reliability of macroeconomic modelling. Other challenges like changes in calculation methods, unavailability of data for certain time periods, and changes in indicator definitions make it hard to train macroeconomic models without overfitting to small datasets. Recent advances in synthetic data generation offer a promising alternative, allowing researchers to create realistic simulations of economic behavior without relying solely on historical data.

\subsection{Goal}
The goal of my senior comprehensive project is to evaluate whether a machine learning model trained on small macroeconomic datasets can be used to generate realistic synthetic time series. By normalizing macroeconomic indicators across different economies, I hope to address the issue of relatively limited time-series for a single country by taking training data from multiple countries. My project will explore whether synthetic time-series produced by a model trained on data from multiple countries could potentially address the problem of lack of data availability, and serve as a valid counterpart to real economic data in forecasting applications.

Abstracting from this project, data availability is becoming an increasingly prevalent issue as the technological world embraces machine learning. Major data companies like Meta and OpenAI have faced lawsuits over data piracy, and smaller companies want to utilize the power of machine learning to drive decision making, but may face issues with security or compliance that prevent them from training models. If we had the power to take a small dataset and extend it, or create realistic alternative datasets using synthetic data generation, then a number of these problems could be solved. Security, privacy, and copyright could be better preserved while still having access to the quantity of data needed to train machine learning models. 

\subsection{Approach}
I will implement a TimeGAN model, which combines a supervised autoencoder to learn temporal dynamics with a generative adversarial network (GAN) to generate realistic synthetic data \textcite{yoon2019timeseriesgenerative}. The model will be trained using macroeconomic data taken from developed, free-market economies, to ensure there is consistency in the underlying macroeconomic trends it identifies. To evaluate whether the generated synthetic data is realistic, I will compare the properties of the synthetic data with real world data using a number of statistical tests focused on distributional similarity, cross time similarity, and novelty. Additionally, I will examine the performance of a forecasting model using the train-synthetic test-real (TSTR) method–training a simple forecasting model on the synthetic data to predict the next step in the real data. Through TSTR evaluation I will be able to assess the potential viability of using synthetic data in training models for real data tasks.

By looking at both statistical factors and forecasting model performance, I hope to have a comprehensive framework for assessing the viability of the synthetic data. I will then iterate over the model, adjusting training parameters in order to see what areas are particularly effective for model optimization on small datasets.

\section{Technical Background}
\subsection{Macroeconomic Indicators}
\subsubsection{Overview}
Macroeconomic indicators are data points which quantify the health of an entire economic market, usually at a country-wide level. They are impactful across all levels of society and business, and are even politically salient, as \textcite{ravallion2021macroeconomicmisery} notes the impact of macroeconomic indicators on electoral outcomes. The macroeconomic indicators that I will focus on will be gross domestic product growth (GDP), inflation rate (I), unemployment rate (R), and population growth (P), which were selected with insight from the research of  \textcite{ravallion2021macroeconomicmisery} and \textcite{baltazar2020sustainableeconomies}. Notably, population growth is not typically considered a macroeconomic indicator, but it helps control for unexplained factors in the other indicators. For example, if GDP and unemployment rates both go up, a possible explanation would be there was a substantial increase in population.

\subsubsection{Selected Indicators}
\begin{itemize}
    \item \textbf{GDP}: measures the entire monetary value of an economy, including all the goods and services produced by it and its constituents, its trade with other countries, and even the value of the intellectual property in the country. I will be using the rate of GDP growth expressed as a percentage change in GDP for this project.
    \item \textbf{Inflation Rate}: is the percent change in price of a pre-defined theoretical ‘basket’ of goods. The ‘basket’ is a collection of common household goods which every individual regardless of income level will regularly purchase.  
    \item \textbf{Unemployment Rate}: is the proportion of the unemployed labor force actively seeking work to the total number of people in the labor market. A high unemployment rate means there are a significant number of individuals without income seeking employment. The efficient unemployment rate is typically around 5\% \cite{voxeuUnemployment}.
    \item \textbf{Population Growth}: is the percent change in the population of study, measured annually
\end{itemize}

\subsubsection{Relevance of Macro-Indicators}
\textcite{ravallion2021macroeconomicmisery} discusses how macroeconomic indicators must be viewed in conjunction with each other, as no individual measure fully captures the state of the economy. When inflation is high, there is usually excess spending in the economy, which increases the amount of goods and services being purchased, which in turn increases GDP. The heightened demand can lead to increased hiring and lower unemployment. Whenever one of the indicators changes, it will surely have an impact on the other indicators and the overall state of the economy. This project will look at each of the indicators outlined above in order to capture a holistic picture of the economy. Macroeconomic forecasting is used by banks, the federal government, and a variety of institutions both in and out of the economic industry. For example, \textcite{baltazar2020sustainableeconomies} detail how macroeconomic models can be used by banks to find their best response to economic shocks, potentially helping avoid situations like the 2008 financial crisis.

\subsection{Relevent Machine Learning Terminology}
\textit{Supervised learning} uses labelled data and produces consistent outputs for given input. While effective for forecasting, the labelling is demanding and it is not well suited for maintaining temporal dynamics in time series generation \cite{yoon2019timeseriesgenerative}.\\
\indent\textit{Unsupervised learning} identifies patterns in unlabelled data. Due to not requiring the labelling step, it is easier to implement than supervised learning, but it can be a high compute ‘black box’ where the internal steps aren’t understood.\\
\indent\textit{Recurrant Neural Networks} are an artificial neural networks that are more suited for processing sequential data. Traditional neural networks pass data between nodes without ‘remembering’ the previous node. RNNs pass data with an additional parameter allowing nodes to observe both the current and previous state in order to learn temporal dynamics.\\
\indent\textit{Generative Adversarial Networks (GANs)} consist of a generator neural network competing with a discriminator to produce synthetic data that the discriminator can’t detect \cite{awsGAN}. As the generative network gets better at generating synthetic data, the discriminator network will become worse at determining the difference between real and synthetic data.\\
\indent\textit{Latent Space} refers to the compression of input data into a lower dimensional space that preserves essential features of the data’s structure \cite{ibmLatentSpace}. \\
\indent\textit{Autoencoders} are the primary neural networks that learn to encode data to a latent space and then reconstruct it, allowing them to learn the structure of the data in the process \cite{ibmLatentSpace}.

\subsection{Choosing TimeGAN}
TimeGAN stands for \textbf{Time-Series Generative Adversarial Network}, a proprietary machine learning model created by \textcite{yoon2019timeseriesgenerative} combining supervised, unsupervised, and adversarial learning to encourage the model to follow the temporal dynamics of training data. The goal of their research stems from the inadequacy of other machine learning models to generate synthetic data that accurately takes temporal correlations into account \cite{yoon2019timeseriesgenerative}. 
Unlike other GANs, TimeGAN is trained on a 3D dataset of multiple independent time-series, shaped as $[N, L, D]$, where $N$ is the number of sequences (e.g., countries), $L$ is the number of time steps, and $D$ is the number of dimensions (e.g., GDP, CPI). This makes TimeGAN perfect for my goal of training a model on multiple countries data.\\

\subsection{TimeGAN Architecture}
The main components of the TimeGAN architecture are pictured in Figure \ref{fig:TimeGAN-Architecture}. Notably the supervisor is not pictured, but operates in the latent space between the embedder and recovery networks. The partial derivates $\delta\mathcal{L}$ represent the loss values used to update each network during training. The training parameters (constant in each training iteration) are represented as $\theta$.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{TimeGAN training.png}
    \caption{Components of TimeGAN Training Architecture. Adapted from \textcite{yoon2019timeseriesgenerative}.}
    \label{fig:TimeGAN-Architecture}
\end{figure}

\textbf{Embedding Network}: The embedding network (\textit{e} in \ref{fig:TimeGAN-Architecture}) Encodes the training data into a lower-dimensional latent space while preserving the temporal relationships. After reducing the dimensionality of the real data, the model is able to learn the structure and dynamics of the data when it is retrieved from the latent space\\
\textbf{Recovery Network}: The recovery network (\textit{r} in \ref{fig:TimeGAN-Architecture}) retrieves or decodes the data embedded in the latent space back into the original data-space. The combined network between the embedding and recovery networks forms the autoencoder section of the TimeGAN\\
\textbf{Supervisor Network}: The supervisor network (not pictured in diagram) works entirely in the latent space learning to predict the next step hidden state from the previous. The supervisor provides its synthetic latent transitions to the Generator Network, so that rather than trying to generate the entire dynamics of the dataset from scratch, the Generator has a per-timestep guidance on how to generate data that matches the supervisors hidden transitions\\
\textbf{Generator Network}: The generator network (\textit{g} in \ref{fig:TimeGAN-Architecture}) generates sequences (time-series) from an input series of a random distribution of Gaussian noise. Through competition with the discriminator and ‘model’ transitions from the supervisor, the generator learns how to better convert the random noise into a realistic distribution.\\
\textbf{Discriminator Network}: The discriminator network (\textit{d} in \ref{fig:TimeGAN-Architecture}) is the adversarial part of the model which attempts to distinguish between real and synthetic generations. Its observations are used to guide the generator to create more realistic output.\\

What makes this model unique is that the embedding and recovery networks together form the autoencoder to refine latent representations, while the supervisor learns time-step shifted latent transformations (the transition from latent representation at time t to time t+1). The autoencoder refined and supervisor time-shifted, latent patterns help the generator to create realistic data for the next time step based on the current time step.The adversarial component of the discriminator provides further reinforcement on generating quality synthetic sequences. The TimeGAN performs these functions described above iteratively, allowing it to improve how it encodes features and generates representations \cite{yoon2019timeseriesgenerative}. The combination of GAN and autoencoder architecture make a TimeGAN the perfect model for my project: optimized to generate realistic time series data.

\subsection{Loss Functions}
\subsubsection{Autoencoder (reconstruction) Loss}
The autoencoder uses mean squared error; a standard loss function in machine learning. The result is a quantification of the average difference between the input into the embedder and output from the recovery network, allowing the autoencoder portion of the model to learn better how to encode and recover data from the latent space.

\begin{equation}
    \mathcal{L}_{\text{AE}} = \frac{1}{L-1}\sum_{t=0}^{L-1}\mathbb{E}\left[\left\|X_t - \hat{X}_t\right\|^2\right]
\end{equation}

First the function takes the squared absolute value of the difference between \(X_t\) and \(\hat{X}_t\), where \(X_t\) is the real data fed into the embedder at time step \(t\), and \(\hat{X}_t\) is the data recovered from the latent embedding by the recovery network. The sum of squared error terms is averaged over the number of time steps \(L\) (the length of the series).

\subsubsection{Supervised Loss}
The supervised loss is also a mean squared error computed over latent representations shifted by one time step. Here, $\hat{H}_{t+1}$ denotes the supervisor network's prediction of the latent state at time $t+1$, and $H_{t+1}$ denotes the true latent state at time $t+1$. The supervised loss therefore encourages the model to learn realistic temporal transitions in the latent space.

\begin{equation}
    \mathcal{L}_{\text{Sup}} = \frac{1}{L-1}\sum_{t=0}^{L-2}\mathbb{E}\left[\left\|H_{t+1} - \hat{H}_{t+1}\right\|^2\right]
\end{equation}

\subsubsection{Generator Loss}
The generator loss has two components. First there is the adversarial loss, which is the component that comes from the discriminators ability to correctly identify the real data from the synthetic. Second is the supervised loss, which is weighted by a gamma coefficient that simply represents the amount of emphasis placed on supervised versus adversarial loss. The loss function is as follows.

\begin{equation}
        \mathcal{L}_{\text{G}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right] + \gamma \mathcal{L}_{\text{Sup}}
\end{equation}

This loss function first takes a logit, which is the raw unbounded output of the discriminator on the synthetic latent sequence $\hat{H}$, and passes it through the sigmoid function which compresses it into a probability from $[0,1]$. A logit is a score, not a probability, so this term can be interpreted as: $\sigma\!\big(D_{\text{logit}}(\hat{H})\big)$ = the probability of seeing a logit value of $D_{\text{logit}}(\hat{H})$. The logit to sigmoid process is done because the discriminator does not output raw probabilities, and internally it makes the derivatives of the loss function much easier to derive. The negative coefficient is due to the fact that loss functions always aim to penalize error rather than reward success. Since the generator is trying to minimize the probability that its synthetic data is labelled synthetic by the generator, the adversarial element of this loss function is negative.

\subsubsection{Discriminator Loss}
The discriminator loss function uses a standard function for yes no classification called binary cross-entropy. Binary cross-entropy is made up of two components, the probability score of assigning fake data the label of fake, and the probability score of assigning real data the label of fake. 

\begin{equation}
    \mathcal{L}_{\text{D}} = -\mathbb{E}\left[\log\left(\sigma\!\left(D_{\text{logit}}\!\left(H\right)\right)\right)\right] - \mathbb{E}\left[\log\left(1 - \sigma\!\left(D_{\text{logit}}\!\left(\hat{H}\right)\right)\right)\right]
\end{equation}

Both the supervisor and discriminator use expectation notation $\mathbb{E}[\cdot]$, which denotes the integral over the data probability distribution, e.g.\ $\mathbb{E}_{x\sim p(x)}[f(x)]=\int f(x)\,p(x)\,dx$, and in practice is approximated by a finite-sample average over a mini-batch: $\mathbb{E}_{x\sim p(x)}[f(x)]\approx\frac{1}{m}\sum_{i=1}^m f(x_i)$, where $m$ is the mini-batch size and $x_i$ are the samples.

\section{Prior Work}

First and foremost, my project relies heavily on the original TimeGAN implementation by \textcite{yoon2019timeseriesgenerative}. They identify that on their own, GANs “do not adequately attend to the temporal correlations unique to time-series data”, while purely supervised models are “inherently deterministic” (\textcite{yoon2019timeseriesgenerative}, 1). Their proposal for addressing this disparity was to create an interwoven framework that provides both the benefits of supervised models and the adversarial component of GANs. However, \textcite{yoon2019timeseriesgenerative} are not the only researchers who aim to optimize the performance of a GAN across temporal features. The unique aspect of their project that stood out to me was the fact that the model is trained on batches of data which is ideal for training the model on macroeconomic conditions using multiple different countries for input. 

Aside from the original TimeGAN repository, my project draws inspiration from studies that reflect the technical, theoretical, and evaluative aspects of synthetic data generation and machine learning applications in economics. The first applies GANs to generating synthetic financial scenarios (\textcite{rizzato2023generativeadversarial}). Another applies a macroeconomic model to loan default rate predictions (\textcite{baltazar2020sustainableeconomies}). Finally, \textcite{yuan2024multifacetedevaluation} and \textcite{livieris2024anevaluationframework} provide a reference for how to go about my evaluation metrics and results discussion.

\subsection{Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation}
\textcite{rizzato2023generativeadversarial} propose a new generative adversarial network approach to synthetic financial scenario generation, which they call Jinkou. Jinkou was made to generate realistic synthetic time series datasets on the movement of equities under different macroeconomic conditions \cite{rizzato2023generativeadversarial}. Unlike the original TimeGAN paper, they introduce a macroeconomic element which offers a similar conceptual approach and methodology to my own goals for this project. 

\subsubsection{Technical Approach}
They first took real datasets on equities with specific variables related to stock performance, and augmented that dataset with global state variables describing macroeconomic conditions \cite{rizzato2023generativeadversarial}. Their proprietary Jinkou method uses a combination of a bidirectional GAN (BiGAN) and conditional GAN (cGAN) to improve the probability distribution of the synthetic data \cite{rizzato2023generativeadversarial}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{bigan_vs_cgan.png}
    \caption{Visual comparison of BiGAN and Conditional GAN training architectures. Adapted from \textcite{rizzato2023generativeadversarial}.}
    \label{fig:bigan-vs-cgan}
\end{figure}

The BiGAN maps latent space to the data space with an autoencoder, similar to the embedder and recovery networks of the TimeGAN. The synthetic time series generated by the trained BiGAN are used as the input for the cGAN, which learns to map each datapoint \textit{y} to a probability score \textit{p} drawn from the dataset distribution of \textit{P} \cite{rizzato2023generativeadversarial}. The cGAN refines the results of the BiGAN by more accurately mapping the data to the probability distribution expected of the data. They perform statistical analysis of their synthetic data generated under four different market conditions (bull market, bear market, volatile market, and debt crisis). They evaluated the synthetic change in stock price from start to end of the time series with the real percent change in stock price \cite{rizzato2023generativeadversarial}. They found similar values for synthetic vs real data, typically with a difference between [-1,2]\%, indicating that their synthetic data was sufficiently accurate to the real data.

\subsubsection{Key Takeaway}
This project demonstrates how synthetic data is a viable way of simulating market scenarios under different macroeconomic conditions. Specific to my project, \textcite{rizzato2023generativeadversarial} found the use of GAN models to be effective for generating synthetic time-series, and were able to compare that synthetic data to real data with a satisfactory degree of accuracy. Their results validate the importance of macroeconomic indicators in financial modelling.

\subsection{Sustainable Economies: Using a Macroeconomic Model to Predict How the Default Rate is Affected Under Economic Stress Scenarios}
\textcite{baltazar2020sustainableeconomies} build a macroeconomic stress-testing framework to predict loan default rates under hypothetical, simulated macroeconomic conditions–a framework they argue would allow banks and policy makers to improve their decision making in economic downturns \textcite{baltazar2020sustainableeconomies}. They use a predefined macroeconomic model rather than machine learning, having that model generate random Monte Carlo simulations \textcite{baltazar2020sustainableeconomies}.\\

Since this is not a machine learning project, my takeaways were not technical but theoretical. They validate the importance of GDP and CPI as revealing macroeconomic indicators, and acknowledge the problem of small datasets for macroeconomic forecasting \textcite{baltazar2020sustainableeconomies}. This study confirmed the relevance and importance of everything I am working on for my project: the use of macroeconomic indicators for understanding economic conditions, the downstream effects of macroeconomic conditions on all aspects of finance and the economy, and the necessity and potential applications of synthetic macroeconomic data in augmenting existing datasets.

\subsection{A Multi-Faceted Evaluation Framework for Assessing Synthetic Data}
This study, by \textcite{yuan2024multifacetedevaluation}, proposes a novel method for the evaluation of synthetic datasets, which they call SynEval. Their evaluation method has three dimensions–fidelity, utility and privacy–of which I am interested in ‘utility’, their measure of the applicability of the synthetic data \cite{yuan2024multifacetedevaluation}.

\subsubsection{Technical Approach}
\indent{}\textcite{yuan2024multifacetedevaluation} refer to the ‘utility’ of a synthetic dataset as its ability to be used downstream for machine learning. This aspect is what I am most concerned with, as my goal is to create synthetic data that is academically and empirically useful. Downstream machine learning refers to the application of the generated data to training/testing machine learning models. The utility evaluation they proposed is based on a framework referred to as TSTR (Train-Synthetic-Test-Real) \cite{yuan2024multifacetedevaluation}. They perform a study on synthetic data generated by popular LLMs ChatGPT 3.5, Claude 3 Opus, and Llama 2 13B. In this case, they used 50 samples from a software product review dataset to train LLM’s to generate synthetic review-rating data \cite{yuan2024multifacetedevaluation}. They then trained four sentiment classification models; one on the real dataset, and the other three on the synthetic datasets produced by the LLMs \cite{yuan2024multifacetedevaluation}. The sentiment classification models were evaluated on unseen test data, with Mean Absolute Error (MAE) and percentage accuracy of correct classifications used to evaluate the performance of each model. 

\subsubsection{Key Takeaway}
In evaluating the synthetic datasets on downstream machine learning performance, they found that the model trained on synthetic data generated by Claude had a MAE of 1.2929 and accuracy of 67.68\%, which was very comparable to the model trained on real data (MAE 1.3019, accuracy 67.92\%) \cite{yuan2024multifacetedevaluation}.\textcite{yuan2024multifacetedevaluation} results prove that synthetic data can be effectively used on downstream machine learning tasks. Their methodology of using downstream machine learning as an evaluation metric for the quality of synthetic data validates my plan of using a forecasting model trained on synthetic versus real data as an evaluation metric. 

\subsection{An Evaluation Framework for Synthetic Data Generation Models}
In the research performed by \textcite{livieris2024anevaluationframework}, they compared a number of different statistical methods specifically for evaluating how realistic synthetic data is to real data. They compare data generated by five synthetic data generation models using standard tests. The three tests they used which I find most relevant to my project were Wasserstein-Cramer's V-test, novelty test, and anomaly detection test \cite{livieris2024anevaluationframework}. 

\begin{itemize}
    \item \textit{Wasserstein-Cramer's V Test}: a test that combines the Wasserstein distance–the quantification of the distance between the probability distributions of the real and synthetic data–with Cramer’s V to account for the distributions across different variables \cite{livieris2024anevaluationframework}. The test evaluates if synthetic data matches the distribution of real data accounting for shape and variance, with low scores being better. Since Wasserstein-Cramer's V Test scores are not bounded to a range, it is somewhat arbitrary determining what is a good score or not. 
    \item \textit{Novelty Test}: a measure of the synthetic data model’s ability to generate new situations not present in the original dataset. It is calculated as the ratio of unmatched instances between the synthetic dataset, where an instance is considered match if $\left| s_i - r_i \right| < \alpha$,
    with $s_i$ representing a synthetic datapoint, $r_i$ representing a real point, and $\alpha$ being some predefined threshold \cite{livieris2024anevaluationframework}. 
    \item \textit{Anomly Detection Test}: an isolation forest is trained on the real data, then applied to the synthetic dataset, to which it assigns an abnormality score to each point \cite{livieris2024anevaluationframework}. A high degree of abnormality would mean that the synthetic dataset has a number of points that do not match the distribution of the real data. An isolation forest is an unsupervised model which isolates anomalies from a dataset using a series of decision trees.
\end{itemize}
\subsubsection{Key Takeaway}
While the anomaly detection test is interesting, I think within my overall framework of evaluating distributional similarity, it is not necessary, as anomalous synthetic data will be clearly visible from other evaluation metrics. The Wasserstein Distance however, I will use as part of my distributional evaluation. Implementing the combination with Cramer’s V-test is not a commonly accepted practice and makes the results a bit less interpretable, so I will focus just on Wasserstein Distance rather than the combined test. Additionally, I will implement the novelty test through a simple nearest neighbors Euclidian distance calculation.

\section{Methods}

\section{Evaluation Metrics}

\section{Results and Discussion}

\section{Ethical Considerations}
As the overall goal of my project was to generate synthetic macroeconomic data capable of being used as a proxy for real data, it is important to assess the ethical implications of such a project. In a hypothetical scenario where synthetic macroeconomic data was to become widely used by economists/bankers, the primary ethical concern with my project becomes the question of who actually benefits from improved economic simulation, and who may be unintentionally excluded from those benefits.

\textcite{ravallion2021macroeconomicmisery} point out how changes in macroeconomic conditions have disproportionately negative impacts on the poorest Americans. Unemployment rate or inflation rates for example, may negatively impact the value of the wealth of upper class Americans, but won’t have any impacts on their daily lives \textcite{ravallion2021macroeconomicmisery}. The poor however, are placed under immense burden from macroeconomic changes.\\

\indent{}The major concern is that improved macroeconomic modelling only serves to benefit those with the financial literacy and wealth to take advantage of it–particularly banks, policymakers, and wealth investors. As such, macroeconomic modelling is likely to be used to benefit these upper-class groups, rather than improve the financial standing and financial literacy of the lower class.

\indent{}There is also the risk that synthetic data is used by a powerful actor (e.g. the Federal Reserve System in deciding interest rates) without disclosure of where that data came from or why it was used. Since macroeconomic factors are impactful to all, anybody who is in a policy making position must be transparent and accountable for the data they use. The general public, and even data-scientists, may not know exactly what patterns an algorithm is learning in training. There is the potential that synthetic data abstracts real world conditions, creating incomplete or untrustworthy datasets that are then used in vital decision making. \\

\indent{}On the topic of ethics in computer science, the use of algorithmic decision making in credit-scoring and loan rating is controversial, as it obscures the process behind making very impactful decisions on people's lives. The use of synthetic macroeconomic data for decision making poses the same risks–obscuring the factors in decisions which impact people's lives. Additionally, if any bias is present in the dataset that is used to produce the synthetic data, then the bias patterns will be present in the synthetic data as well. In loan rating for example, it is possible that if you come from a demographic which more frequently has their loan applications rejected, then that bias will carry into the algorithm. If a synthetic data production model was trained on the same input data, then the synthetic results would also carry that bias, and overly reject loan applicants from said demographic. When it comes to real people's lives, using synthetic data becomes a real liability.

\section{Conclusion and Future Work}
Throughout this project I was able to make notable improvements in the performance of the TimeGAN model on generating synthetic data from a small input dataset. Both the statistical and TSTR scores improved from my v0 to v14 tests. However, I was unable to fully prevent overfitting to the input data and the tendency to collapse toward the mean. This is due, as I discovered, to the inherent conflict between generating novel, varied distributions, and matching the statistical properties of the original dataset. I was able to refine this tradeoff to an extent, and produce synthetic data in v14 which demonstrated both better novelty and distributional scores than in v0. Ultimately though I would conclude that in its current state, the size of the training dataset is too severe a limiting factor in the ability to produce temporally realistic and distributionally viable synthetic datasets using TimeGAN.

If a model were to reach the goal of producing realistic synthetic macroeconomic data to the extent that said synthetic data was indistinguishable from real, then this raises a number of ethical concerns. Aside from the obvious like fabricating data, if applied to macroeconomic modelling techniques there is a risk of further exacerbating the divide between those who have the knowledge and wealth to take advantage of financial information and those who do not. Additionally, if synthetic data is used to help inform or train decision making in financial experts, there is the risk that they make decisions that impact real people's lives based on a made up scenario. Transparency on the generation process of synthetic data is vital to mitigating these risks.


% Defer appendix material until after the bibliography so the bibliography
% appears as part of the main paper and the following sections become Appendix.
\AtEndDocument{%
    \clearpage
    \appendix
    \section{Replication Instructions}
    \section{Architecture Overview}
}

\printbibliography

\clearpage

\onecolumn

\end{document}